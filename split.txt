- ON THE POWER-LAW HESSIAN SPECTRUMS IN DEEP LEARNING
- Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data
- Weight decay induces low-rank attention layers

llama 7b
magnitude_ww 192.69  
wanda_ww 22.77 | 22.24 0.2 ww | 22.13 0.21 ww   | 24 GT
sparsegpt 19.67 | 18.49 0.2 ww |  GT-19.326          18.72 0.162                           


llama 7b
QKV output m 198.12 | QKV     | OUT     |V      |OUT V  |K      |Q      |Q OUT GATE |Gate    |QK     |QK OUT |QKVoutg|QKVout + split | GT
           w 22.01  | w 24.33 | w 51.33 |w 29.95|w 31.33|w 26.78|w 27.37|w  26.04   |w 108.09|w 26.85|w 22.99|w 22.21|m 228.5        |
           s 18.75  |         |         |       |       |       |       |           |         |       |       |      |w 21.69        |w 23.52 0.22ww | 21.47 QKVout + split
                                                                                                                     |s 17.76        |

zero_shot evaluation results
sparse QKVout + split
{'results': {'arc_challenge': {'acc': 0.27474402730375425, 'acc_stderr': 0.013044617212771227, 'acc_norm': 0.28754266211604096, 'acc_norm_stderr': 0.013226719056266132}, 'rte': {'acc': 0.5992779783393501, 'acc_stderr': 0.029497229237163147}, 'winogrande': {'acc': 0.6322020520915549, 'acc_stderr': 0.013552385559833591}, 'openbookqa': {'acc': 0.184, 'acc_stderr': 0.01734617478175285, 'acc_norm': 0.316, 'acc_norm_stderr': 0.020812359515855867}, 'hellaswag': {'acc': 0.37651862178848833, 'acc_stderr': 0.004835222794006518, 'acc_norm': 0.49611631149173474, 'acc_norm_stderr': 0.004989630887066196}, 'boolq': {'acc': 0.6788990825688074, 'acc_stderr': 0.008166123126572139}, 'arc_easy': {'acc': 0.4671717171717172, 'acc_stderr': 0.010237645778853844, 'acc_norm': 0.44823232323232326, 'acc_norm_stderr': 0.010204645126856931}}, 'versions': {'arc_challenge': 0, 'rte': 0, 'winogrande': 0, 'openbookqa': 0, 'hellaswag': 0, 'boolq': 1, 'arc_easy': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=pinkmanlove/llama-7b-hf,cache_dir=./llm_weights', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}

MEAN: 0.4597 0.2 

wanda_ww
{'results': {'arc_challenge': {'acc': 0.2508532423208191, 'acc_stderr': 0.012668198621315433, 'acc_norm': 0.2832764505119454, 
'acc_norm_stderr': 0.013167478735134575}, 'winogrande': {'acc': 0.6156274664561957, 'acc_stderr': 0.013671567600836196}, 
'rte': {'acc': 0.6137184115523465, 'acc_stderr': 0.029307720385270512}, 'arc_easy': {'acc': 0.4675925925925926, 
'acc_stderr': 0.010238210368801884, 'acc_norm': 0.45664983164983164, 'acc_norm_stderr': 0.01022114965011818}, 
'boolq': {'acc': 0.6541284403669725, 'acc_stderr': 0.00831919840241541}, 'openbookqa': {'acc': 0.174, 'acc_stderr': 0.016971271257516147, 
'acc_norm': 0.298, 'acc_norm_stderr': 0.02047511809298897}, 'hellaswag': {'acc': 0.3648675562636925, 'acc_stderr': 0.0048040917088125615, 
'acc_norm': 0.4757020513841864, 'acc_norm_stderr': 0.004983886091690529}}, 'versions': {'arc_challenge': 0, 'winogrande': 0, 'rte': 0, 
'arc_easy': 0, 'boolq': 1, 'openbookqa': 0, 'hellaswag': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 
'pretrained=pinkmanlove/llama-7b-hf,cache_dir=./llm_weights', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 
'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}

 MEAN: 0.4487 0.215 wanda 

LLaMA 2 (7B) V Entropy (Avg): 7.6507
LLaMA 7B V Entropy (Avg): 7.6593
LLaMA 2 (7B) V Rank (Avg): 1.0000
LLaMA 7B V Rank (Avg): 1.0000
LLaMA 2 (7B) QK Entropy (Avg): 4.3091
LLaMA 7B QK Entropy (Avg): 3.9172



llama 2 7b
wanda_ww 30.1 0.25 ww | 32.06 GT | 29.62 split 0.3ww
magnitdue_ww    2845 0.02 ww | nan GT 
sparsegpt 20.99 0.2 ww | 21.51 GT      

llama 2 7b
GT      |QKVout  |QK out  |Q out   |Q out gate|QK out split | QK out split 7b
m nan   |        |        |        |          |             |
w 31.94 |w 32.93 |w 29.89 |w 31.47 |w  33.91  |w 30.00      | 29.39
s 21.51 |        |        |        |          |             |

llama 13b
GT     | QKVout |
m 2384 |m 1591  |
w 14.50|w 13.99 |
s 13.14|s 12.86 |

sparse 0.29
{'results': {'arc_challenge': {'acc': 0.28242320819112626, 'acc_stderr': 0.013155456884097222, 'acc_norm': 0.302901023890785, 'acc_norm_stderr': 0.013428241573185347}, 'openbookqa': {'acc': 0.228, 'acc_stderr': 0.018781306529363204, 'acc_norm': 0.338, 'acc_norm_stderr': 0.02117566569520941}, 'arc_easy': {'acc': 0.54503367003367, 'acc_stderr': 0.01021808445460259, 'acc_norm': 0.5214646464646465, 'acc_norm_stderr': 0.010250325159456652}, 'hellaswag': {'acc': 0.4202350129456284, 'acc_stderr': 0.0049258777057711875, 'acc_norm': 0.5688109938259311, 'acc_norm_stderr': 0.004942302768002106}, 'rte': {'acc': 0.49458483754512633, 'acc_stderr': 0.030094698123239966}, 'boolq': {'acc': 0.6865443425076453, 'acc_stderr': 0.008113624272232311}, 'winogrande': {'acc': 0.6866614048934491, 'acc_stderr': 0.013036512096747978}}, 'versions': {'arc_challenge': 0, 'openbookqa': 0, 'arc_easy': 0, 'hellaswag': 0, 'rte': 0, 'boolq': 1, 'winogrande': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=pinkmanlove/llama-13b-hf,cache_dir=./llm_weights', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
 0.4810

llama 3 8b

GT    |QKVout  |QKV      |Q out   |Q out Gate |Q out split |Q out gate split|
219402|m       |m        |m 54888 |m          |            |                |
106.56|w 110.9 |w 115.22 |w 71.79 |w 76.65    |w   78.62   |w          71.21|
37.26 |s       |s        |s 32.22 |s 31.79    |            |s          31.22|


llama 3.1 8b
Dense |GT m3ws2 |Q out   |Qoutgate|
6.237 |m 294449 |m 166623|115743  |
6.237 |w 103.41 |w 67.97 |65.78   |
6.237 |s 38.85  |s 31.45 |31.02   |

Layer 0: Self-Attention (q_proj) error vs MP: 5.8229, MLP (gate_proj) error vs MP: 0.5482
Layer 1: Self-Attention (q_proj) error vs MP: 2.8727, MLP (gate_proj) error vs MP: 0.4139
Layer 2: Self-Attention (q_proj) error vs MP: 2.8257, MLP (gate_proj) error vs MP: 0.3806
Layer 3: Self-Attention (q_proj) error vs MP: 2.5908, MLP (gate_proj) error vs MP: 0.3984
Layer 4: Self-Attention (q_proj) error vs MP: 2.4098, MLP (gate_proj) error vs MP: 0.3975
Layer 5: Self-Attention (q_proj) error vs MP: 1.9537, MLP (gate_proj) error vs MP: 0.3795
Layer 6: Self-Attention (q_proj) error vs MP: 1.8795, MLP (gate_proj) error vs MP: 0.3888
Layer 7: Self-Attention (q_proj) error vs MP: 1.8027, MLP (gate_proj) error vs MP: 0.3867
Layer 8: Self-Attention (q_proj) error vs MP: 1.6238, MLP (gate_proj) error vs MP: 0.3732
Layer 9: Self-Attention (q_proj) error vs MP: 1.5670, MLP (gate_proj) error vs MP: 0.3816
Layer 10: Self-Attention (q_proj) error vs MP: 1.5267, MLP (gate_proj) error vs MP: 0.3896
Layer 11: Self-Attention (q_proj) error vs MP: 1.7370, MLP (gate_proj) error vs MP: 0.3789
Layer 12: Self-Attention (q_proj) error vs MP: 1.4941, MLP (gate_proj) error vs MP: 0.3812
Layer 13: Self-Attention (q_proj) error vs MP: 1.4659, MLP (gate_proj) error vs MP: 0.3877
Layer 14: Self-Attention (q_proj) error vs MP: 1.5243, MLP (gate_proj) error vs MP: 0.3953
Layer 15: Self-Attention (q_proj) error vs MP: 1.5273, MLP (gate_proj) error vs MP: 0.4067
Layer 16: Self-Attention (q_proj) error vs MP: 1.6298, MLP (gate_proj) error vs MP: 0.3945
Layer 17: Self-Attention (q_proj) error vs MP: 1.5179, MLP (gate_proj) error vs MP: 0.3957
Layer 18: Self-Attention (q_proj) error vs MP: 1.5643, MLP (gate_proj) error vs MP: 0.4045
Layer 19: Self-Attention (q_proj) error vs MP: 1.5882, MLP (gate_proj) error vs MP: 0.4112
Layer 20: Self-Attention (q_proj) error vs MP: 1.6473, MLP (gate_proj) error vs MP: 0.4109
Layer 21: Self-Attention (q_proj) error vs MP: 1.5764, MLP (gate_proj) error vs MP: 0.4118
Layer 22: Self-Attention (q_proj) error vs MP: 1.6064, MLP (gate_proj) error vs MP: 0.4118
Layer 23: Self-Attention (q_proj) error vs MP: 1.5916, MLP (gate_proj) error vs MP: 0.4132
Layer 24: Self-Attention (q_proj) error vs MP: 1.5419, MLP (gate_proj) error vs MP: 0.4166
Layer 25: Self-Attention (q_proj) error vs MP: 1.6004, MLP (gate_proj) error vs MP: 0.4204
Layer 26: Self-Attention (q_proj) error vs MP: 1.4903, MLP (gate_proj) error vs MP: 0.4235
Layer 27: Self-Attention (q_proj) error vs MP: 1.5254, MLP (gate_proj) error vs MP: 0.4254
Layer 28: Self-Attention (q_proj) error vs MP: 1.4706, MLP (gate_proj) error vs MP: 0.4250
Layer 29: Self-Attention (q_proj) error vs MP: 1.3716, MLP (gate_proj) error vs MP: 0.4223
Layer 30: Self-Attention (q_proj) error vs MP: 1.4395, MLP (gate_proj) error vs MP: 0.4158
Layer 31: Self-Attention (q_proj) error vs MP: 1.5978, MLP (gate_proj) error vs MP: 0.3553

Layer    Var(Q+Output)    Var(QKV+Output)
Layer  0:      6.8023          66.6098
Layer  1:      1.7505          22.4268
Layer  2:      0.6707          12.0442
Layer  3:      0.9012          15.7111
Layer  4:      1.0241          19.9928
Layer  5:      0.6653          14.2648
Layer  6:      0.7762          15.2454
Layer  7:      0.6908          17.3287
Layer  8:      0.5712          18.1904
Layer  9:      0.9602          23.8098
Layer 10:      0.6082          17.6028
Layer 11:      0.5814          20.4818
Layer 12:      0.8822          22.0152
Layer 13:      0.6521          22.6965
Layer 14:      0.5942          23.1866
Layer 15:      1.4302          23.8024
Layer 16:      0.9522          19.9324
Layer 17:      1.2952          29.7267
Layer 18:      0.9494          22.1502
Layer 19:      0.9797          19.3142
Layer 20:      0.8644          20.4155
Layer 21:      0.9271          22.1338
Layer 22:      0.8117          19.7737
Layer 23:      0.9121          20.3354
Layer 24:      1.0460          24.4832
Layer 25:      1.0835          21.7994
Layer 26:      1.1706          30.2282
Layer 27:      1.1668          32.5548
Layer 28:      1.5212          36.6760
Layer 29:      1.5352          43.5886
Layer 30:      2.9982          64.9272
Layer 31:      4.1792          72.9902

==== Summary ====
All layers Q+Output variances: [6.8022594  1.7505128  0.67073023 0.9011826  1.0241171  0.66527545
 0.7761883  0.69081646 0.57116586 0.96019024 0.60822624 0.58143175
 0.8822267  0.6520713  0.594187   1.4302163  0.95221716 1.2951628
 0.94935375 0.97971964 0.8644248  0.9270747  0.8116518  0.9120609
 1.0460224  1.0835354  1.1706367  1.1667795  1.5212361  1.5351686
 2.9982014  4.179174  ]
All layers Attention-only mean variances: [102.31417    11.067372    4.540181    4.8845205   5.200709    4.8718896
   5.200384    5.5359073   5.3908963   5.493868    5.024547    4.4261317
   5.589787    6.3783855   5.7296386   6.0082498   5.6517253   5.9606986
   5.498638    4.133563    4.15825     4.3731494   3.5758412   3.427335
   3.3134153   3.0430374   3.790486    3.5733676   3.479709    3.215975
   4.594109    5.704971 ]
All layers MLP-only mean variances: [ 1.8593284  1.3342737  1.3491153  1.5887476  1.8701531  1.906558
  2.032314   2.170205   2.2507017  2.4496467  2.373629   2.4527009
  2.3661978  2.3773997  2.3708098  2.5378225  2.4494166  2.3578682
  2.2054212  2.0457609  2.0141258  1.9576955  1.944918   1.9099833
  1.9090347  2.0394032  2.1584115  2.524278   2.6009033  2.676144
  3.672923  13.955382 ]
All layers Attention+MLP mean variances: [59.262093   6.8960443  3.1725814  3.4720464  3.773328   3.601033
  3.84264    4.0934634  4.045099   4.189202   3.8884392  3.5803754
  4.2082486  4.663677   4.2901406  4.5209236  4.2793074  4.4166284
  4.08726    3.2387908  3.2393396  3.337955   2.8768742  2.7770412
  2.711538   2.6129086  3.0910258  3.1237576  3.1030781  2.9846187
  4.199315   9.240862 ]