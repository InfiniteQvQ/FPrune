import torch
import torch.nn as nn
from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM

# **åŠ è½½ LLaMA 7B æ¨¡å‹**
cache_dir = "/root/autodl-tmp/llm_weights"
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir=cache_dir,
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer_name = "HuggingFaceM4/llama-7b-tokenizer"
tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name)

device = "cuda" if torch.cuda.is_available() else "cpu"


# **æµ‹è¯•è¾“å…¥**
sample_text = "The quick brown fox jumps over the lazy dog."
inputs = tokenizer(sample_text, return_tensors="pt").to(device)

# **å‰å‘ä¼ æ’­ï¼Œè·å–åŸºå‡†è¾“å‡º**
with torch.no_grad():
    base_output = model(**inputs).logits

# **å®šä¹‰æ‰°åŠ¨å¹…åº¦**
delta = 1e-3

# **å­˜å‚¨å½±å“åˆ†æ•°**
influence_scores = {}

# **è®¡ç®—æ¯ä¸€å±‚çš„å½±å“åˆ†æ•°**
for layer_idx, layer in enumerate(model.model.layers):
    print(f"Processing Layer {layer_idx}...")

    # éœ€è¦è®¡ç®—çš„æƒé‡
    components = {
        "Q": layer.self_attn.q_proj,
        "K": layer.self_attn.k_proj,
        "V": layer.self_attn.v_proj,
        "Output": layer.self_attn.o_proj,
        "Gate": layer.mlp.gate_proj,
        "Up": layer.mlp.up_proj,
        "Down": layer.mlp.down_proj,
    }

    for name, param in components.items():
        original_weight = param.weight.data.clone()

        # **æ‰°åŠ¨æƒé‡**
        param.weight.data += delta

        # **å‰å‘ä¼ æ’­**
        with torch.no_grad():
            perturbed_output = model(**inputs).logits

        # **è®¡ç®—å½±å“åˆ†æ•°**
        influence_score = torch.norm(base_output - perturbed_output).item() / delta
        influence_scores[f"Layer {layer_idx} - {name}"] = influence_score

        # **æ¢å¤åŸå§‹æƒé‡**
        param.weight.data.copy_(original_weight)

# **æ˜¾ç¤ºå½±å“åˆ†æ•°**
sorted_scores = sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)

# **æ‰“å°å‰ 10 ä¸ªæœ€é‡è¦çš„å±‚**
print("\nğŸ”¥ Top-10 Most Important Layers ğŸ”¥")
for name, score in sorted_scores[:10]:
    print(f"{name}: {score:.6f}")

# **æ‰“å°å‰ 10 ä¸ªæœ€ä¸é‡è¦çš„å±‚ï¼ˆé€‚åˆå‰ªæï¼‰**
print("\nâ„ï¸ Top-10 Least Important Layers (Pruning Candidates) â„ï¸")
for name, score in sorted_scores[-10:]:
    print(f"{name}: {score:.6f}")
