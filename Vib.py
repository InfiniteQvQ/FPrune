import torch
import numpy as np
from transformers import AutoModelForCausalLM

# âœ… è‡ªåŠ¨é€‰æ‹© GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… åŠ è½½ LLaMA-7B
cache_dir = "/root/autodl-tmp/llm_weights"
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir=cache_dir,
    device_map="auto",
    torch_dtype=torch.bfloat16
).to(device)  # ç¡®ä¿æ•´ä¸ªæ¨¡å‹åŠ è½½åˆ° GPU

def singular_value_spectrum(weight_matrix):
    """è®¡ç®— SVD å¥‡å¼‚å€¼è°±ï¼ˆåœ¨ GPU è¿è¡Œï¼‰"""
    weight_matrix = weight_matrix.to(device).float()  # ç¡®ä¿åœ¨ GPU ä¸Š
    U, S, V = torch.linalg.svd(weight_matrix, full_matrices=False)  # ç›´æ¥åœ¨ GPU è®¡ç®— SVD
    return S.detach().cpu().numpy()  # è®¡ç®—å®Œæˆåè½¬æ¢ä¸º NumPy æ•°ç»„

def esd_spectrum(weight_matrix):
    """è®¡ç®—ç‰¹å¾å€¼è°±åˆ†å¸ƒ (ESD)ï¼ˆåœ¨ GPU è¿è¡Œï¼‰"""
    weight_matrix = weight_matrix.to(device).float()  # ç¡®ä¿åœ¨ GPU ä¸Š
    gram_matrix = weight_matrix @ weight_matrix.T  # è®¡ç®— Gram çŸ©é˜µ
    eigenvalues, _ = torch.linalg.eigh(gram_matrix)  # GPU è®¡ç®—ç‰¹å¾å€¼
    return eigenvalues.abs().detach().cpu().numpy()  # è®¡ç®—å®Œæˆåè½¬æ¢ä¸º NumPy æ•°ç»„

layer_importance_scores = {}

for layer_idx, layer in enumerate(model.model.layers):
    print(f"Processing Layer {layer_idx}...")

    # ğŸ§  Attention å±‚
    q_proj = layer.self_attn.q_proj.weight.to(device)
    k_proj = layer.self_attn.k_proj.weight.to(device)
    v_proj = layer.self_attn.v_proj.weight.to(device)
    attn_score = np.mean([
        np.sum(singular_value_spectrum(q_proj)), 
        np.sum(singular_value_spectrum(k_proj)), 
        np.sum(singular_value_spectrum(v_proj))
    ])  # SVD è®¡ç®—é‡è¦æ€§

    # ğŸ”¥ MLP å±‚
    gate_proj = layer.mlp.gate_proj.weight.to(device)
    up_proj = layer.mlp.up_proj.weight.to(device)
    down_proj = layer.mlp.down_proj.weight.to(device)
    mlp_score = np.mean([
        np.sum(esd_spectrum(gate_proj)), 
        np.sum(esd_spectrum(up_proj)), 
        np.sum(esd_spectrum(down_proj))
    ])  # ESD è®¡ç®—é‡è¦æ€§

    # ğŸ¯ Output å±‚
    output_proj = layer.self_attn.o_proj.weight.to(device)
    output_score = np.sum(singular_value_spectrum(output_proj))  # SVD è®¡ç®—é‡è¦æ€§

    # ğŸ“Š è®¡ç®—ç›¸å¯¹é‡è¦æ€§
    layer_relative_importance = attn_score + mlp_score + output_score
    layer_importance_scores[layer_idx] = layer_relative_importance

# ğŸš€ æ’åº
sorted_layers = sorted(layer_importance_scores.items(), key=lambda x: x[1], reverse=True)

print("\nğŸ” LLaMA 7B æ¯å±‚çš„ç›¸å¯¹é‡è¦æ€§æ’åº:")
for idx, importance in sorted_layers:
    print(f"Layer {idx}: {importance:.4f}")
