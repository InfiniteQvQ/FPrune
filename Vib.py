import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaConfig
from datasets import load_dataset
from torch.utils.data import DataLoader
import math

# ğŸš€ **VIB å¯è®­ç»ƒå‰ªæ Mask**
class VIBMask(nn.Module):
    def __init__(self, size, pruning_ratio=0.5):
        super().__init__()
        self.mu = nn.Parameter(torch.zeros(size))  # éœ€è¦æ¢¯åº¦
        self.sigma = nn.Parameter(torch.ones(size) * pruning_ratio)  # éœ€è¦æ¢¯åº¦

    def forward(self, prev_mask=None):
        """ è®¡ç®—å‰ªæ maskï¼ŒåŒæ—¶è€ƒè™‘å‰é¢å±‚çš„ token ä¾èµ–æ€§ """
        epsilon = torch.randn_like(self.sigma)
        mask = torch.sigmoid(self.mu + epsilon * self.sigma)

        # ğŸš€ **è®©å½“å‰å±‚å‰ªæ Mask å—å‰é¢å±‚å½±å“**
        if prev_mask is not None:
            mask = mask * prev_mask

        return mask

    def kl_loss(self):
        """ è®¡ç®— KL æ•£åº¦ lossï¼Œè®©å‰ªæ Mask å¯è®­ç»ƒ """
        return -0.5 * torch.mean(1 + self.sigma - self.mu ** 2 - torch.exp(self.sigma))


# ğŸš€ **Llama Self Attentionï¼ˆæ”¯æŒå‰ªæï¼‰**
class LlamaAttention(nn.Module):
    def __init__(self, config: LlamaConfig, pruning_ratio=0.5):
        super().__init__()
        self.num_heads = config.num_attention_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.head_dim = config.hidden_size // self.num_heads
        self.hidden_size = config.hidden_size

        # çº¿æ€§å˜æ¢
        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)
        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)

        # ğŸš€ **GQA å‰ªæ Mask**
        self.mask_q = VIBMask(self.num_heads, pruning_ratio)  # Query å¤´å‰ªæ Mask
        self.mask_kv = VIBMask(self.num_key_value_heads, pruning_ratio)  # Key/Value å¤´å‰ªæ Mask

    def forward(self, hidden_states, attention_mask=None):
        batch_size, seq_length, _ = hidden_states.shape

        # è®¡ç®— Q/K/V
        query_states = self.q_proj(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim)
        key_states = self.k_proj(hidden_states).view(batch_size, seq_length, self.num_key_value_heads, self.head_dim)
        value_states = self.v_proj(hidden_states).view(batch_size, seq_length, self.num_key_value_heads, self.head_dim)

        # ğŸš€ **å‰ªæ Q å¤´**
        mask_q = self.mask_q()
        query_states = query_states * mask_q

        # ğŸš€ **å‰ªæ KV å¤´**
        mask_kv = self.mask_kv()
        key_states = key_states * mask_kv
        value_states = value_states * mask_kv

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask

        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, value_states)

        attn_output = attn_output.view(batch_size, seq_length, self.hidden_size)
        attn_output = self.o_proj(attn_output)

        # ğŸš€ **è¿”å› KL Loss**
        return attn_output, self.mask_q.kl_loss() + self.mask_kv.kl_loss()


# ğŸš€ **å‰ªæ Llama3**
def prune_llama_model(llama_model, pruning_ratios):
    """ ğŸš€ TVA-Prune å‰ªæ Llama3 æ¨¡å‹ """
    for i, layer in enumerate(llama_model.model.layers):  
        layer.self_attn.mask_q = VIBMask(layer.self_attn.num_heads, pruning_ratios[i])
        layer.self_attn.mask_kv = VIBMask(layer.self_attn.num_key_value_heads, pruning_ratios[i])
    return llama_model


# ğŸš€ **åŠ è½½æ•°æ®é›†**
def get_dataloader():
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B")
    dataset = load_dataset("openwebtext", split="train")

    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_datasets.set_format(type="torch", columns=["input_ids", "attention_mask"])

    return DataLoader(tokenized_datasets, batch_size=4, shuffle=True)


# ğŸš€ **è®­ç»ƒå‰ªæ Mask**
def train_mask(model, dataloader, epochs=3, lr=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # ğŸš€ **å†»ç»“ Llama3 çš„æ‰€æœ‰æƒé‡**
    for param in model.parameters():
        param.requires_grad = False

    # ğŸš€ **åªè®­ç»ƒå‰ªæ Mask**
    vib_params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(vib_params, lr=lr)

    for epoch in range(epochs):
        model.train()
        total_kl_loss = 0

        for step, batch in enumerate(dataloader):
            batch = {k: v.to(device) for k, v in batch.items()}
            _, kl_loss = model(**batch)  # ğŸš€ **åªè®¡ç®— KL æŸå¤±**

            loss = kl_loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            total_kl_loss += loss.item()
            if step % 50 == 0:
                print(f"Epoch {epoch+1}, Step {step}, KL Loss: {loss.item()}")

        print(f"Epoch {epoch+1} finished, Avg KL Loss: {total_kl_loss / len(dataloader)}")


if __name__ == "__main__":
    # ğŸš€ åŠ è½½ Llama3
    cache_dir = "/root/autodl-tmp/llm_weights"
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", cache_dir=cache_dir,device_map="auto", torch_dtype=torch.float16)

    # ğŸš€ è®¾ç½®å‰ªææ¯”ä¾‹
    pruning_ratios = [0.7 * i for i in range(32)]

    # ğŸš€ å‰ªææ¨¡å‹
    model = prune_llama_model(model, pruning_ratios)

    # ğŸš€ è®­ç»ƒå‰ªæ Mask
    train_dataloader = get_dataloader()
    train_mask(model, train_dataloader)
