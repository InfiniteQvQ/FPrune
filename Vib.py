import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import math

# ğŸš€ VIB å¯è®­ç»ƒå‰ªæ Mask
class VIBMask(nn.Module):
    def __init__(self, size, pruning_ratio=0.5):
        super().__init__()
        self.mu = nn.Parameter(torch.zeros(size))  # å¯è®­ç»ƒå‚æ•°
        self.sigma = nn.Parameter(torch.ones(size) * pruning_ratio)

    def forward(self, prev_mask=None):
        epsilon = torch.randn_like(self.sigma)
        mask = torch.sigmoid(self.mu + epsilon * self.sigma)
        if prev_mask is not None:
            mask = mask * prev_mask
        return mask

    def kl_loss(self):
        return -0.5 * torch.mean(1 + self.sigma - self.mu ** 2 - torch.exp(self.sigma))


# ğŸš€ Llama Self-Attentionï¼ˆæ”¯æŒå‰ªæï¼‰
class LlamaAttention(nn.Module):
    def __init__(self, config, pruning_ratio=0.5):
        super().__init__()
        self.num_heads = config.num_attention_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.head_dim = config.hidden_size // self.num_heads
        self.hidden_size = config.hidden_size

        # çº¿æ€§å˜æ¢
        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)
        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)

        # ğŸš€ GQA å‰ªæ Maskï¼šé’ˆå¯¹ Query å’Œ Key/Value åˆ†åˆ«æ·»åŠ å¯è®­ç»ƒ Mask
        self.mask_q = VIBMask(self.num_heads, pruning_ratio)
        self.mask_kv = VIBMask(self.num_key_value_heads, pruning_ratio)

    def forward(self, hidden_states, attention_mask=None):
        batch_size, seq_length, _ = hidden_states.shape

        # è®¡ç®— Q/K/V
        query_states = self.q_proj(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim)
        key_states = self.k_proj(hidden_states).view(batch_size, seq_length, self.num_key_value_heads, self.head_dim)
        value_states = self.v_proj(hidden_states).view(batch_size, seq_length, self.num_key_value_heads, self.head_dim)

        # ğŸš€ å‰ªæ Query å¤´
        mask_q = self.mask_q()
        query_states = query_states * mask_q

        # ğŸš€ å‰ªæ Key/Value å¤´
        mask_kv = self.mask_kv()
        key_states = key_states * mask_kv
        value_states = value_states * mask_kv

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, value_states)

        attn_output = attn_output.view(batch_size, seq_length, self.hidden_size)
        attn_output = self.o_proj(attn_output)

        # è¿”å›è¾“å‡ºå’Œ Mask çš„ KL æŸå¤±
        return attn_output, self.mask_q.kl_loss() + self.mask_kv.kl_loss()


# ğŸš€ Llama MLPï¼ˆæ”¯æŒå‰ªæï¼‰
class LlamaMLP(nn.Module):
    def __init__(self, config, pruning_ratio=0.5):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size

        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size)

        # ä¸º gate, up, down æ·»åŠ å‰ªæ Mask
        self.mask_gate = VIBMask(config.intermediate_size, pruning_ratio)
        self.mask_up = VIBMask(config.intermediate_size, pruning_ratio)
        self.mask_down = VIBMask(config.hidden_size, pruning_ratio)

    def forward(self, hidden_states):
        # å¯¹ gate_proj å’Œ up_proj å‰ªæ
        mask_gate = self.mask_gate()
        mask_up = self.mask_up()
        hidden_states = F.silu(self.gate_proj(hidden_states) * mask_gate) * (self.up_proj(hidden_states) * mask_up)
        # å¯¹ down_proj å‰ªæ
        mask_down = self.mask_down()
        hidden_states = self.down_proj(hidden_states) * mask_down

        # è¿”å›è¾“å‡ºå’Œä¸‰ä¸ª mask çš„ KL æŸå¤±ä¹‹å’Œ
        return hidden_states, self.mask_gate.kl_loss() + self.mask_up.kl_loss() + self.mask_down.kl_loss()


# ğŸš€ Llama Decoder Layerï¼ˆæ”¯æŒå‰ªæï¼‰
class LlamaDecoderLayer(nn.Module):
    def __init__(self, config, layer_idx, pruning_ratio=0.5):
        super().__init__()
        self.attention = LlamaAttention(config, pruning_ratio)
        self.mlp = LlamaMLP(config, pruning_ratio)
        self.norm_1 = nn.LayerNorm(config.hidden_size)
        self.norm_2 = nn.LayerNorm(config.hidden_size)

    def forward(self, hidden_states, attention_mask=None):
        residual = hidden_states
        hidden_states = self.norm_1(hidden_states)
        hidden_states, kl_attn = self.attention(hidden_states, attention_mask)
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = self.norm_2(hidden_states)
        hidden_states, kl_mlp = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        # ç´¯åŠ æ³¨æ„åŠ›å’Œ MLP éƒ¨åˆ†çš„ KL æŸå¤±
        return hidden_states, kl_attn + kl_mlp


# å¦‚æœä½ æƒ³ä»é›¶æ„å»ºæ¨¡å‹ï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰ LlamaModelï¼›
# ä½†è¿™é‡Œæˆ‘ä»¬å‡è®¾ä½¿ç”¨é¢„è®­ç»ƒçš„ LlamaForCausalLMï¼Œå¹¶é€šè¿‡å‰ªæå‡½æ•°æ›¿æ¢å…¶å±‚ä¸­çš„ Maskã€‚
def prune_llama_model(model, pruning_ratios):
    """
    æ ¹æ®æ¯å±‚åˆ†é…çš„å‰ªææ¯”ä¾‹ï¼Œæ›¿æ¢é¢„è®­ç»ƒæ¨¡å‹ä¸­æ¯å±‚æ³¨æ„åŠ›å’Œ MLP éƒ¨åˆ†çš„å‰ªæ Maskã€‚
    æ³¨æ„ï¼šé¢„è®­ç»ƒ LlamaForCausalLM çš„ Decoder å±‚ä¸­æ³¨æ„åŠ›æ¨¡å—å±æ€§ä¸º `self_attn`ï¼Œè€Œéæˆ‘ä»¬è‡ªå®šä¹‰çš„ `attention`ï¼Œ
    æ‰€ä»¥è¿™é‡Œéœ€è¦ç”¨ `layer.self_attn` å’Œ `layer.mlp` æ¥è®¿é—®ã€‚
    """
    for i, layer in enumerate(model.model.layers):
        pruning_ratio = pruning_ratios[i]
        # æ›¿æ¢æ³¨æ„åŠ›ä¸­çš„å‰ªæ Mask
        layer.self_attn.mask_q = VIBMask(layer.self_attn.num_heads, pruning_ratio)
        layer.self_attn.mask_kv = VIBMask(layer.self_attn.num_key_value_heads, pruning_ratio)
        # æ›¿æ¢ MLP ä¸­çš„å‰ªæ Mask
        layer.mlp.mask_gate = VIBMask(layer.mlp.intermediate_size, pruning_ratio)
        layer.mlp.mask_up = VIBMask(layer.mlp.intermediate_size, pruning_ratio)
        layer.mlp.mask_down = VIBMask(layer.mlp.hidden_size, pruning_ratio)
    return model


# ğŸš€ åŠ è½½æ•°æ®é›†ï¼ˆè¿™é‡Œä½¿ç”¨ wikitext-2-raw-v1ï¼‰
def get_dataloader():
    cache_dir = "/root/autodl-tmp"  # æŒ‡å®šç¼“å­˜ç›®å½•
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B", cache_dir=cache_dir)
    # å¦‚æœæ²¡æœ‰ pad_tokenï¼Œåˆ™ç”¨ eos_token ä»£æ›¿
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train", cache_dir=cache_dir)

    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

    return DataLoader(tokenized_dataset, batch_size=4, shuffle=True)


# ğŸš€ è®­ç»ƒå‰ªæ Maskï¼ˆå†»ç»“ Llama æƒé‡ï¼Œä»…è®­ç»ƒ Maskï¼‰
def train_mask(model, dataloader, epochs=3, lr=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    # å†»ç»“æ¨¡å‹æ‰€æœ‰æƒé‡ï¼Œåªè®­ç»ƒ Mask å‚æ•°
    for param in model.parameters():
        param.requires_grad = False
    vib_params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(vib_params, lr=lr)

    for epoch in range(epochs):
        model.train()
        total_kl_loss = 0
        for step, batch in enumerate(dataloader):
            batch = {k: v.to(device) for k, v in batch.items()}
            # å‰å‘ä¼ æ’­åªè¿”å› (outputs, kl_loss)
            _, kl_loss = model(**batch)
            loss = kl_loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            total_kl_loss += loss.item()
            if step % 50 == 0:
                print(f"Epoch {epoch+1}, Step {step}, KL Loss: {loss.item()}")
        print(f"Epoch {epoch+1} finished, Avg KL Loss: {total_kl_loss / len(dataloader)}")


if __name__ == "__main__":
    cache_dir = "/root/autodl-tmp/llm_weights"
    # ğŸš€ åŠ è½½é¢„è®­ç»ƒ LlamaForCausalLM æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.1-8B",
        cache_dir=cache_dir,
        device_map="auto",
        torch_dtype=torch.float16
    )

    # ğŸš€ ç”Ÿæˆæ¯å±‚å‰ªææ¯”ä¾‹ï¼Œè¿™é‡Œç¤ºä¾‹ä½¿ç”¨ [0, 0.7*1, 0.7*2, ..., 0.7*31]
    pruning_ratios = [0.7 * i for i in range(model.config.num_hidden_layers)]
    # å¦‚æœæ¨¡å‹å±‚æ•°ä¸è¶³32ï¼Œåˆ™æŒ‰å®é™…å±‚æ•°è®¾ç½®

    # ğŸš€ å¯¹é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨å‰ªæï¼ˆæ›¿æ¢å„å±‚ Maskï¼‰
    model = prune_llama_model(model, pruning_ratios)

    # ğŸš€ åŠ è½½æ•°æ®é›†
    dataloader = get_dataloader()

    # ğŸš€ è®­ç»ƒå‰ªæ Maskï¼ˆå†»ç»“ Llama æƒé‡ï¼Œä»…è®­ç»ƒ Mask å‚æ•°ï¼‰
    train_mask(model, dataloader)

    # è®­ç»ƒå®Œæˆåå¯ä»¥ä¿å­˜æ¨¡å‹
    model.save_pretrained("/root/autodl-tmp/pruned_llama3_8b")
