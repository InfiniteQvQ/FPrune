import torch
import numpy as np
from transformers import AutoModelForCausalLM

# ğŸ› ï¸ åŠ è½½ LLaMA-7B
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir="/root/autodl-tmp/llm_weights",
    device_map="auto",
    torch_dtype=torch.float16
)

# ğŸ¯ è®¡ç®— PL_Alpha_Hill
def pl_alpha_hill(weight_matrix, k_ratio=0.1):
    """è®¡ç®— Hill ä¼°è®¡çš„ PL_Alpha_Hill"""
    weight_matrix = weight_matrix.float()
    with torch.no_grad():
        eigvals = torch.linalg.eigvalsh(weight_matrix @ weight_matrix.T).cpu().numpy()
    eigvals = np.sort(eigvals)[::-1]  # é™åºæ’åˆ—
    n = len(eigvals)
    k = int(k_ratio * n)  # å–å‰ k% è®¡ç®—
    if k < 1: return 1.0
    lambda_n_k = eigvals[k-1]
    log_ratio = np.log(eigvals[:k]) - np.log(lambda_n_k)
    alpha_hill = 1 + k / np.sum(log_ratio)
    return alpha_hill

# ğŸ¯ è®¡ç®— ESDï¼ˆæœ€å¤§ç‰¹å¾å€¼ï¼‰
def esd_spectrum(weight_matrix):
    """è®¡ç®—æœ€å¤§ç‰¹å¾å€¼ (ESD)"""
    weight_matrix = weight_matrix.float()
    with torch.no_grad():
        eigvals = torch.linalg.eigvalsh(weight_matrix @ weight_matrix.T)
    return eigvals.max().cpu().numpy()  # è¿”å›æœ€å¤§ç‰¹å¾å€¼

# ğŸ¯ è®¡ç®—å•å±‚é‡è¦æ€§
def process_layer(layer_idx, layer, lambda_esd=1.0):
    print(f"Processing Layer {layer_idx}...")

    # ğŸ”¥ MLP å±‚ï¼ˆGate, Up, Downï¼‰- è®¡ç®— ESD
    mlp_esd = (
        esd_spectrum(layer.mlp.gate_proj.weight)+
        esd_spectrum(layer.mlp.up_proj.weight)+
        esd_spectrum(layer.mlp.down_proj.weight)
    )

    # ğŸ§  Q, K, V, Output å±‚ï¼ˆè®¡ç®— Alpha-Hill ä¹‹å’Œï¼‰
    attn_hill_sum = (
        pl_alpha_hill(layer.self_attn.q_proj.weight) +
        pl_alpha_hill(layer.self_attn.k_proj.weight) +
        pl_alpha_hill(layer.self_attn.v_proj.weight) +
        pl_alpha_hill(layer.self_attn.o_proj.weight) + 
        pl_alpha_hill(layer.mlp.gate_proj.weight) + 
        pl_alpha_hill(layer.mlp.up_proj.weight) +
        pl_alpha_hill(layer.mlp.down_proj.weight)
    )
    print("attn sum: ", np.log(1 + attn_hill_sum), " mlp  ", np.log(1 + mlp_esd))
    # ğŸ“Š è®¡ç®—ç›¸å¯¹é‡è¦æ€§ (ESD - Alpha-Hill)
    layer_relative_importance =  np.log(1 + attn_hill_sum)
    return layer_idx, layer_relative_importance

# ğŸš€ è®¡ç®—æ‰€æœ‰å±‚çš„é‡è¦æ€§
lambda_esd = 1  # å¯ä»¥è°ƒæ•´è¿™ä¸ªå‚æ•°
layer_importance_scores = [process_layer(idx, layer, lambda_esd) for idx, layer in enumerate(model.model.layers)]

# ğŸš€ å½’ä¸€åŒ–
scores = torch.tensor([imp[1] for imp in layer_importance_scores])
s1, s2 = 0.8, 1.2
max_score, min_score = scores.max(), scores.min()
normalized_scores = ((scores - min_score) / (max_score - min_score)) * (s2 - s1) + s1

# è°ƒæ•´å‡å€¼åˆ° 0.7
scale = 0.7 / normalized_scores.mean()
normalized_scores = normalized_scores * scale
print(normalized_scores.mean())
# æ‰“å°æœ€ç»ˆç»“æœ
print("\nğŸ” LLaMA 7B æ¯å±‚çš„å½’ä¸€åŒ–ç›¸å¯¹é‡è¦æ€§:")
res = []
for (idx, _), importance in zip(layer_importance_scores, normalized_scores.tolist()):
    print(f"Layer {idx}: {importance:.4f}")
    res.append(importance)
print(res)

