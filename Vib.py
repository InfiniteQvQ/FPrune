import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from datasets import load_dataset
import os

# âœ… æ¯å±‚çš„å‰ªæç‡
TARGET_SPARSITY_PER_LAYER = [0.7] * 7 * 32


# âœ… Variational Information Bottleneck (VIB) Mask å±‚
class VIBLayer(nn.Module):
    def __init__(self, input_dim, target_sparsity):
        super().__init__()
        self.mu = nn.Parameter(torch.randn(input_dim) * 0.01)
        self.log_sigma = nn.Parameter(torch.randn(input_dim) * 0.01)
        self.target_sparsity = target_sparsity

    def forward(self, x):
        std = torch.exp(0.5 * self.log_sigma)
        eps = torch.randn_like(std)
        z = self.mu + eps * std
        mask = torch.sigmoid(z)  # ç”Ÿæˆ soft mask

        # è®¡ç®—é˜ˆå€¼ï¼Œä½¿å‰ªæç‡ç¬¦åˆ target_sparsity
        sorted_mask, _ = torch.sort(mask)
        threshold = sorted_mask[int(len(mask) * self.target_sparsity)]
        final_mask = (mask > threshold).float()
        return x * final_mask, mask  # è¿”å›å‰ªæåçš„å¼ é‡ & åŸå§‹ mask


# âœ… é‡æ–°å°è£… LLaMAï¼Œæ·»åŠ  **æ¯å±‚ç‹¬ç«‹** çš„ VIB å±‚
class PrunedLlama(nn.Module):
    def __init__(self, model, target_sparsity_per_layer):
        super().__init__()
        self.model = model
        self.num_layers = len(model.model.layers)

        # ä¸ºæ¯ä¸€å±‚çš„ MLP å’Œ Attention åˆ›å»ºç‹¬ç«‹çš„ Mask
        self.mlp_vib_layers = nn.ModuleList([
            VIBLayer(model.model.layers[i].mlp.down_proj.weight.shape[0], target_sparsity_per_layer[i])
            for i in range(self.num_layers)
        ])

        self.attn_vib_layers = nn.ModuleList([
            VIBLayer(model.model.layers[i].self_attn.o_proj.weight.shape[0], target_sparsity_per_layer[i])
            for i in range(self.num_layers)
        ])

    def forward(self, x):
        kl_total = 0  # ç»Ÿè®¡ KL Loss
        for i, layer in enumerate(self.model.model.layers):
            x, mask_mlp = self.mlp_vib_layers[i](x)
            x, mask_attn = self.attn_vib_layers[i](x)
            x = layer(x)  # è¿›å…¥ LLaMA åŸå§‹ç»“æ„

            # è®¡ç®— KL æŸå¤±
            kl_total += kl_loss(self.mlp_vib_layers[i].mu, self.mlp_vib_layers[i].log_sigma)
            kl_total += kl_loss(self.attn_vib_layers[i].mu, self.attn_vib_layers[i].log_sigma)

        return x, kl_total


# âœ… KL çº¦æŸ Loss
def kl_loss(mu, log_sigma):
    return -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.exp())


def train_mask(rank, world_size):
    # 1ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

    # 2ï¸âƒ£ åŠ è½½ LLaMA æ¨¡å‹
    model_name = "pinkmanlove/llama-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")
    cache_dir = "/root/autodl-tmp/llm_weights"
    
    # âœ… è®© Hugging Face å¤„ç† GPU åˆ†é…ï¼Œé¿å… OOM
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        cache_dir=cache_dir,
        torch_dtype=torch.float16,
        device_map="auto"  # âœ… è®© Hugging Face è‡ªåŠ¨åˆ†é… GPU
    )

    print(f"ğŸ”¥ Model has {len(model.model.layers)} layers")
    print(f"ğŸ”¥ Target sparsity list has {len(TARGET_SPARSITY_PER_LAYER)} values")

    # 3ï¸âƒ£ åˆå§‹åŒ–å‰ªææ¨¡å‹ï¼ˆä¸å†ä½¿ç”¨ DDPï¼‰
    pruned_model = PrunedLlama(model, TARGET_SPARSITY_PER_LAYER).to(rank)

    # 4ï¸âƒ£ è®­ç»ƒ
    optimizer = optim.Adam(pruned_model.parameters(), lr=5e-4)
    beta = 1e-5

    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    train_texts = dataset["text"]
    train_tokens = tokenizer(train_texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
    train_loader = DataLoader(train_tokens["input_ids"], batch_size=4, shuffle=True)

    for epoch in range(5):  # è®­ç»ƒ 5 è½®
        total_loss = 0
        for inputs in train_loader:
            inputs = inputs.to(rank)

            optimizer.zero_grad()
            outputs = pruned_model(inputs)

            # è®¡ç®— KL æŸå¤±
            kl_losses = sum(kl_loss(layer.mu, layer.log_sigma) for layer in pruned_model.mlp_vib_layers)
            kl_losses += sum(kl_loss(layer.mu, layer.log_sigma) for layer in pruned_model.attn_vib_layers)

            loss = outputs.loss + beta * kl_losses
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"[GPU {rank}] Epoch {epoch + 1}, Loss: {total_loss:.4f}")

    # 6ï¸âƒ£ ä»…åœ¨ä¸»è¿›ç¨‹ä¿å­˜ Mask
    if rank == 0:
        torch.save(pruned_model.state_dict(), "pruned_llama7b.pth")
        print("[GPU 0] ğŸ¯ Mask å·²ä¿å­˜ï¼")

    # 7ï¸âƒ£ å…³é—­è¿›ç¨‹ç»„
    dist.destroy_process_group()



# âœ… è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
def main():
    world_size = torch.cuda.device_count()  # è·å– GPU æ•°é‡
    print(f"ğŸ’¡ å‘ç° {world_size} å¼  GPUï¼Œå¯åŠ¨å¤šå¡è®­ç»ƒ...")

    mp.spawn(train_mask, args=(world_size,), nprocs=world_size, join=True)


if __name__ == "__main__":
    main()
