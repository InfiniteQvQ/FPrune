import torch
import numpy as np
from transformers import AutoModelForCausalLM
import math
# ğŸ› ï¸ åŠ è½½ LLaMA-7B
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir="/root/autodl-tmp/llm_weights",
    device_map="auto",
    torch_dtype=torch.float16
)

# ğŸ¯ è®¡ç®— PL_Alpha_Hill
def pl_alpha_hill_peak(weight_matrix, bins=100, EVALS_THRESH=1e-5, conv_norm=0.5, filter_zeros=False):
    """
    ä½¿ç”¨ä¸ net_esd_estimator (fix_fingers='xmin_peak') ä¸€è‡´çš„æ–¹å¼è®¡ç®— PL_Alpha_Hillï¼ˆalphahillï¼‰çš„å€¼ã€‚
    
    å‚æ•°:
      weight_matrix: å¾…è®¡ç®—çš„æƒé‡çŸ©é˜µï¼ˆä¾‹å¦‚ layer.self_attn.q_proj.weightï¼‰
      bins: ç›´æ–¹å›¾ç®±æ•°ï¼Œé»˜è®¤ 100
      EVALS_THRESH: è¿‡æ»¤è¿‘é›¶æˆ–è´Ÿç‰¹å¾å€¼çš„é˜ˆå€¼ï¼Œé»˜è®¤ 1e-5
      conv_norm: é’ˆå¯¹ Conv2d å±‚å½’ä¸€åŒ–å› å­ï¼Œé»˜è®¤ 0.5ï¼ˆå¯¹äº Linear å±‚å¯å¿½ç•¥ï¼‰
      filter_zeros: æ˜¯å¦ä»…ä¿ç•™å¤§äºé˜ˆå€¼çš„ç‰¹å¾å€¼ï¼Œé»˜è®¤ Falseï¼›
                    å½“ False æ—¶ï¼Œå°†æ‰€æœ‰å°äºé˜ˆå€¼çš„å€¼æ›¿æ¢ä¸ºé˜ˆå€¼ï¼Œé¿å… log è¿ç®—å‡ºé”™ã€‚
                    
    è¿”å›:
      final_alphahat: å½’ä¸€åŒ–åçš„ alphahill æ•°å€¼ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸º final_alpha * log10(spectral_norm)
    """
    weight_matrix = weight_matrix.float()
    
    with torch.no_grad():
        eigvals = torch.linalg.eigvalsh(weight_matrix @ weight_matrix.T).cpu()
    
    # å¯¹ç‰¹å¾å€¼æŒ‰å‡åºæ’åº
    eigvals, _ = torch.sort(eigvals, descending=False)
    spectral_norm = eigvals[-1].item()  # æœ€å¤§ç‰¹å¾å€¼
    fnorm = torch.sum(eigvals).item()
    
    # å¤„ç†ç‰¹å¾å€¼ï¼Œé˜²æ­¢å–å¯¹æ•°æ—¶å‡ºç°éæœ‰é™æ•°å€¼
    if filter_zeros:
        nz_eigs = eigvals[eigvals > EVALS_THRESH]
        if nz_eigs.numel() == 0:
            nz_eigs = eigvals
    else:
        # å°†æ‰€æœ‰å°äº EVALS_THRESH çš„å€¼æ›¿æ¢ä¸º EVALS_THRESHï¼Œé˜²æ­¢å‡ºç° 0 æˆ–è´Ÿæ•°
        nz_eigs = torch.clamp(eigvals, min=EVALS_THRESH)
    N = nz_eigs.numel()
    
    # è®¡ç®—è‡ªç„¶å¯¹æ•°å’Œ log10ï¼ˆç”¨äºç›´æ–¹å›¾ï¼‰
    log_nz_eigs = torch.log(nz_eigs)
    hist_nz_eigs = torch.log10(nz_eigs)
    
    # æ£€æŸ¥å¯¹æ•°ç»“æœæ˜¯å¦æœ‰æ•ˆ
    if not torch.isfinite(hist_nz_eigs).all():
        raise ValueError("log10(eigvals) åŒ…å«éæœ‰é™å€¼ï¼Œè¯·æ£€æŸ¥ç‰¹å¾å€¼å¤„ç†é€»è¾‘ã€‚")
    
    min_e, max_e = hist_nz_eigs.min(), hist_nz_eigs.max()
    if not (torch.isfinite(min_e) and torch.isfinite(max_e)):
        raise ValueError("è®¡ç®—å‡ºçš„ log10(eigvals) çš„æœ€å°å€¼æˆ–æœ€å¤§å€¼ä¸ºéæœ‰é™å€¼ã€‚")
    
    # æ„é€ ç›´æ–¹å›¾ï¼Œé€‰æ‹©ç›´æ–¹å›¾å¯†åº¦æœ€å¤§çš„ç®±å¯¹åº”çš„ xmin2
    counts = torch.histc(hist_nz_eigs, bins=bins, min=min_e.item(), max=max_e.item())
    boundaries = torch.linspace(min_e, max_e, bins + 1)
    ih = torch.argmax(counts).item()
    xmin2 = 10 ** boundaries[ih].item()
    
    # è®¾ç½® xmin é™åˆ¶èŒƒå›´ï¼šxmin_min ä¸º log10(0.95*xmin2)ï¼Œxmin_max ä¸º 1.5*xmin2
    xmin_min = torch.log10(0.95 * xmin2)
    xmin_max = 1.5 * xmin2
    
    # éå†å€™é€‰çš„ xmin å€¼ï¼Œè®¡ç®— alpha åŠæ‹ŸåˆæŒ‡æ ‡ D
    alphas = torch.zeros(N - 1)
    Ds = torch.ones(N - 1)
    
    for i, xmin in enumerate(nz_eigs[:-1]):
        # ä¸åŸå®ç°ä¸€è‡´ï¼Œä½¿ç”¨ xmin_peak ç­›é€‰å€™é€‰ xmin
        if xmin < xmin_min:
            continue
        if xmin > xmin_max:
            break
        
        n = float(N - i)
        seq = torch.arange(n, dtype=torch.float32)
        # è®¡ç®— alpha = 1 + n / (sum(log(candidate_tail)) - n * log(candidate))
        alpha = 1 + n / (torch.sum(log_nz_eigs[i:]) - n * log_nz_eigs[i])
        alphas[i] = alpha
        if alpha > 1:
            D = torch.max(torch.abs(1 - (nz_eigs[i:] / xmin) ** (-alpha + 1) - seq / n))
            Ds[i] = D
    
    min_D_index = torch.argmin(Ds).item()
    final_alpha = alphas[min_D_index].item()
    
    # ç”¨è°±èŒƒæ•°çš„ log10 å¯¹ alpha å½’ä¸€åŒ–ï¼Œå¾—åˆ°æœ€ç»ˆçš„ alphahill
    final_alphahat = final_alpha * math.log10(spectral_norm)
    
    return final_alphahat

# ğŸ¯ è®¡ç®— ESDï¼ˆæœ€å¤§ç‰¹å¾å€¼ï¼‰
def esd_spectrum(weight_matrix):
    """è®¡ç®—æœ€å¤§ç‰¹å¾å€¼ (ESD)"""
    weight_matrix = weight_matrix.float()
    with torch.no_grad():
        eigvals = torch.linalg.eigvalsh(weight_matrix @ weight_matrix.T)
    return eigvals.max().cpu().numpy()  # è¿”å›æœ€å¤§ç‰¹å¾å€¼

# ğŸ¯ è®¡ç®—å•å±‚é‡è¦æ€§
def process_layer(layer_idx, layer):
    print(f"Processing Layer {layer_idx}...")

    # ğŸ§  è®¡ç®— Q, K, V, O å±‚çš„ Alpha-Hill ä¹‹å’Œ
    attn_hill_sum = (
        pl_alpha_hill_peak(layer.self_attn.q_proj.weight) +
        pl_alpha_hill_peak(layer.self_attn.k_proj.weight) +
        pl_alpha_hill_peak(layer.self_attn.v_proj.weight) +
        pl_alpha_hill_peak(layer.self_attn.o_proj.weight) + 
        pl_alpha_hill_peak(layer.mlp.gate_proj.weight) + 
        pl_alpha_hill_peak(layer.mlp.up_proj.weight) + 
        pl_alpha_hill_peak(layer.mlp.down_proj.weight) 
    )
    
   
    print(attn_hill_sum)
    return layer_idx, attn_hill_sum


# ğŸš€ è®¡ç®—æ‰€æœ‰å±‚çš„é‡è¦æ€§
lambda_esd = 1  # å¯ä»¥è°ƒæ•´è¿™ä¸ªå‚æ•°
layer_importance_scores = [process_layer(idx, layer) for idx, layer in enumerate(model.model.layers)]

# ğŸš€ å½’ä¸€åŒ–
scores = torch.tensor([imp[1] for imp in layer_importance_scores])
s1, s2 = 0.8, 1.2
max_score, min_score = scores.max(), scores.min()
normalized_scores = ((scores - min_score) / (max_score - min_score)) * (s2 - s1) + s1

# è°ƒæ•´å‡å€¼åˆ° 0.7
scale = 0.7 / normalized_scores.mean()
normalized_scores = normalized_scores * scale
print(normalized_scores.mean())
# æ‰“å°æœ€ç»ˆç»“æœ
print("\nğŸ” LLaMA 7B æ¯å±‚çš„å½’ä¸€åŒ–ç›¸å¯¹é‡è¦æ€§:")
res = []
for (idx, _), importance in zip(layer_importance_scores, normalized_scores.tolist()):
    print(f"Layer {idx}: {importance:.4f}")
    res.append(importance)
print(res)

