import torch
import numpy as np
from transformers import AutoModelForCausalLM

# ðŸ› ï¸ åŠ è½½ LLaMA-7B
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir="/root/autodl-tmp/llm_weights",
    device_map="auto",
    torch_dtype=torch.float16
)

# ðŸŽ¯ è®¡ç®— PL_Alpha_Hill
def pl_alpha_hill_peak(weight_matrix, bins=100):
    """
    ä½¿ç”¨ 'xmin_peak' æ–¹æ³•è®¡ç®— PL_Alpha_Hillï¼ˆalphahillï¼‰çš„å€¼

    å‚æ•°ï¼š
      weight_matrix: æƒé‡çŸ©é˜µï¼ˆä¾‹å¦‚ layer.self_attn.q_proj.weightï¼‰
      bins: ç”¨äºŽç›´æ–¹å›¾çš„ç®±æ•°ï¼ˆé»˜è®¤ 100ï¼‰

    è¿”å›žï¼š
      final_alphahat: å½’ä¸€åŒ–åŽçš„ alphahill æ•°å€¼
    """
    weight_matrix = weight_matrix.float()
    with torch.no_grad():
        # è®¡ç®— matrix @ matrix.T çš„ç‰¹å¾å€¼ï¼Œå¾—åˆ°å®žæ•°ç‰¹å¾å€¼
        eigvals = torch.linalg.eigvalsh(weight_matrix @ weight_matrix.T).cpu().numpy()

    # å°†ç‰¹å¾å€¼æŒ‰å‡åºæŽ’åˆ—
    eigvals = np.sort(eigvals)

    # è¿‡æ»¤æŽ‰é›¶å€¼å¹¶å–å¯¹æ•°ï¼Œæž„é€  log-scale ç›´æ–¹å›¾
    positive_eigs = eigvals[eigvals > 0]
    if len(positive_eigs) == 0:
        return 1.0  # å¦‚æžœæ²¡æœ‰æ­£ç‰¹å¾å€¼ï¼Œè¿”å›žé»˜è®¤å€¼
    log_nz_eigs = np.log10(positive_eigs)
    min_e, max_e = log_nz_eigs.min(), log_nz_eigs.max()

    # æž„é€ ç›´æ–¹å›¾å¹¶é€‰æ‹©ç›´æ–¹å›¾å¯†åº¦æœ€å¤§çš„ç®±å¯¹åº”çš„ xmin
    counts, bin_edges = np.histogram(log_nz_eigs, bins=bins, range=(min_e, max_e))
    peak_idx = np.argmax(counts)
    xmin = 10 ** bin_edges[peak_idx]

    # è®¾ç½® xmin çš„é™åˆ¶èŒƒå›´ï¼Œé¿å…æžç«¯æƒ…å†µ
    xmin_min = 0.95 * xmin
    xmin_max = 1.5 * xmin

    # ç­›é€‰å‡ºå¤„äºŽ [xmin, xmin_max] èŒƒå›´å†…çš„ç‰¹å¾å€¼
    valid_eigs = eigvals[(eigvals >= xmin) & (eigvals <= xmin_max)]
    n = len(valid_eigs)
    if n < 2:
        return 1.0  # ç‰¹å¾å€¼å¤ªå°‘æ—¶è¿”å›žé»˜è®¤å€¼

    # éåŽ†ä¸åŒå€™é€‰ xmin å€¼ï¼Œè®¡ç®—å¯¹åº”çš„ alpha å’Œæ‹ŸåˆæŒ‡æ ‡ D
    alphas = []
    Ds = []
    for i, current_xmin in enumerate(valid_eigs[:-1]):
        tail = valid_eigs[i:]
        alpha = 1 + len(tail) / np.sum(np.log(tail / current_xmin))
        alphas.append(alpha)
        D = np.max(np.abs(1 - (tail / current_xmin) ** (-alpha + 1) - np.arange(len(tail)) / len(tail)))
        Ds.append(D)

    # é€‰æ‹©ä½¿ D æœ€å°çš„ alpha
    min_D_index = np.argmin(Ds)
    final_alpha = alphas[min_D_index]

    # ä½¿ç”¨è°±èŒƒæ•°å½’ä¸€åŒ–å¾—åˆ°æœ€ç»ˆ alphahill
    spectral_norm = np.max(eigvals)
    final_alphahat = final_alpha * np.log10(spectral_norm)

    return final_alphahat

# ðŸŽ¯ è®¡ç®— ESDï¼ˆæœ€å¤§ç‰¹å¾å€¼ï¼‰
def esd_spectrum(weight_matrix):
    """è®¡ç®—æœ€å¤§ç‰¹å¾å€¼ (ESD)"""
    weight_matrix = weight_matrix.float()
    with torch.no_grad():
        eigvals = torch.linalg.eigvalsh(weight_matrix @ weight_matrix.T)
    return eigvals.max().cpu().numpy()  # è¿”å›žæœ€å¤§ç‰¹å¾å€¼

# ðŸŽ¯ è®¡ç®—å•å±‚é‡è¦æ€§
def process_layer(layer_idx, layer):
    print(f"Processing Layer {layer_idx}...")

    # ðŸ§  è®¡ç®— Q, K, V, O å±‚çš„ Alpha-Hill ä¹‹å’Œ
    attn_hill_sum = (
        pl_alpha_hill_peak(layer.self_attn.q_proj.weight) +
        pl_alpha_hill_peak(layer.self_attn.k_proj.weight) +
        pl_alpha_hill_peak(layer.self_attn.v_proj.weight) +
        pl_alpha_hill_peak(layer.self_attn.o_proj.weight) + 
        pl_alpha_hill_peak(layer.mlp.gate_proj.weight) + 
        pl_alpha_hill_peak(layer.mlp.up_proj.weight) + 
        pl_alpha_hill_peak(layer.mlp.down_proj.weight) 
    )
    
   
    print(attn_hill_sum)
    return layer_idx, attn_hill_sum


# ðŸš€ è®¡ç®—æ‰€æœ‰å±‚çš„é‡è¦æ€§
lambda_esd = 1  # å¯ä»¥è°ƒæ•´è¿™ä¸ªå‚æ•°
layer_importance_scores = [process_layer(idx, layer) for idx, layer in enumerate(model.model.layers)]

# ðŸš€ å½’ä¸€åŒ–
scores = torch.tensor([imp[1] for imp in layer_importance_scores])
s1, s2 = 0.8, 1.2
max_score, min_score = scores.max(), scores.min()
normalized_scores = ((scores - min_score) / (max_score - min_score)) * (s2 - s1) + s1

# è°ƒæ•´å‡å€¼åˆ° 0.7
scale = 0.7 / normalized_scores.mean()
normalized_scores = normalized_scores * scale
print(normalized_scores.mean())
# æ‰“å°æœ€ç»ˆç»“æžœ
print("\nðŸ” LLaMA 7B æ¯å±‚çš„å½’ä¸€åŒ–ç›¸å¯¹é‡è¦æ€§:")
res = []
for (idx, _), importance in zip(layer_importance_scores, normalized_scores.tolist()):
    print(f"Layer {idx}: {importance:.4f}")
    res.append(importance)
print(res)

