import torch
from transformers import AutoModelForCausalLM, LlamaTokenizer

# ğŸ”¹ åŠ è½½ LLaMA-7B æ¨¡å‹
cache_dir = "/root/autodl-tmp/llm_weights"
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir=cache_dir,
    device_map="auto",
    torch_dtype=torch.float16
)
tokenizer = LlamaTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")

# ğŸ”¹ å­˜å‚¨æ¢¯åº¦ Ã— æ¿€æ´»å€¼
grad_activation_scores = {}

def forward_hook(module, input, output):
    """ å­˜å‚¨å‰å‘ä¼ æ’­çš„æ¿€æ´»å€¼ """
    layer_name = module._get_name()
    
    # ğŸ”¹ å…¼å®¹ tuple è¾“å‡ºï¼ˆé€šå¸¸ LlamaDecoderLayer è¿”å›å¤šä¸ªå€¼ï¼‰
    if isinstance(output, tuple):
        hidden_states = output[0]  # å– hidden_states
    else:
        hidden_states = output

    grad_activation_scores[layer_name] = {"activation": hidden_states.detach()}

def backward_hook(module, grad_input, grad_output):
    """ è®¡ç®—æ¢¯åº¦ Ã— æ¿€æ´»å€¼ """
    layer_name = module._get_name()
    
    # ğŸ”¹ å…¼å®¹ tuple çš„ grad_output
    if isinstance(grad_output, tuple):
        gradient = grad_output[0].detach()
    else:
        gradient = grad_output.detach()

    activation = grad_activation_scores[layer_name]["activation"]
    
    # è®¡ç®—è´¡çŒ®åº¦
    contribution = (gradient * activation).mean().item()
    grad_activation_scores[layer_name]["contribution"] = contribution

# ğŸ”¹ ç»‘å®šå‰å‘ & åå‘ä¼ æ’­ Hook
hooks = []
for layer_id, layer in enumerate(model.model.layers):
    fwd_hook = layer.register_forward_hook(forward_hook)
    bwd_hook = layer.register_full_backward_hook(backward_hook)
    hooks.extend([fwd_hook, bwd_hook])

# ğŸ”¹ è¿è¡Œæ¨¡å‹å¹¶è®¡ç®—æ¢¯åº¦
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
text = "Artificial Intelligence is transforming the world with LLaMA-7B."
inputs = tokenizer(text, return_tensors="pt").to(device)

# ğŸ”¹ è®¡ç®— Loss å¹¶åå‘ä¼ æ’­
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
loss.backward()

# ğŸ”¹ é‡Šæ”¾ Hooks
for hook in hooks:
    hook.remove()

# ğŸ”¹ æå–å¹¶æ’åºè´¡çŒ®åº¦
sorted_grad_activations = sorted(
    [(name, data["contribution"]) for name, data in grad_activation_scores.items() if "contribution" in data],
    key=lambda x: -x[1]
)

# ğŸ”¹ æ‰“å°ç»“æœ
print("\nğŸš€ **æ¢¯åº¦ Ã— æ¿€æ´»å€¼ è´¡çŒ®åº¦ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰** ğŸš€\n")
for layer, score in sorted_grad_activations:
    print(f"{layer}: Contribution={score:.6f}")
