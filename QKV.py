import torch
from transformers import AutoModelForCausalLM, LlamaTokenizer

# ğŸ”¹ ç¡®å®šè®¾å¤‡æ•°é‡
device_count = torch.cuda.device_count()
device_map = {i: f"cuda:{i}" for i in range(device_count)}  # å¤š GPU æ˜ å°„

print(f"ğŸš€ Using {device_count} GPUs: {device_map}")

# ğŸ”¹ åŠ è½½ LLaMA-7Bï¼Œå¹¶è‡ªåŠ¨åˆ†é…å¤šä¸ª GPU
cache_dir = "/root/autodl-tmp/llm_weights"
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir=cache_dir,
    torch_dtype=torch.float16,
    device_map="auto",  # ğŸš€ è®© Hugging Face è‡ªåŠ¨åˆ†é…å¤šä¸ª GPU
)

tokenizer = LlamaTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")

# ğŸ”¹ å­˜å‚¨æ¿€æ´»å€¼å’Œæ¢¯åº¦ï¼ˆè·¨ GPUï¼‰
grad_activation_scores = {}

def forward_hook(module, input, output):
    """å­˜å‚¨å‰å‘ä¼ æ’­çš„æ¿€æ´»å€¼"""
    layer_name = module._get_name()

    # âœ… å…¼å®¹ tuple è¾“å‡º
    if isinstance(output, tuple):
        hidden_states = output[0]
    else:
        hidden_states = output

    # âœ… å­˜å‚¨æ¿€æ´»å€¼ï¼Œå¹¶ç¡®ä¿å®ƒå­˜å‚¨åœ¨è®¡ç®—çš„ GPU ä¸Š
    grad_activation_scores[layer_name] = {
        "activation": hidden_states.detach().to(hidden_states.device)  # ç¡®ä¿å­˜å‚¨åœ¨å½“å‰è®¡ç®—çš„ GPU ä¸Š
    }

def backward_hook(module, grad_input, grad_output):
    """è®¡ç®—æ¢¯åº¦ Ã— æ¿€æ´»å€¼"""
    layer_name = module._get_name()

    # âœ… å…¼å®¹ tuple çš„ `grad_output`
    if isinstance(grad_output, tuple):
        gradient = grad_output[0].detach()
    else:
        gradient = grad_output.detach()

    activation = grad_activation_scores[layer_name]["activation"]

    # ğŸ”¹ ğŸš€ å…³é”®ä¿®æ­£ï¼šç¡®ä¿ `gradient` å’Œ `activation` åœ¨åŒä¸€ä¸ª GPU ä¸Š
    if gradient.device != activation.device:
        activation = activation.to(gradient.device)

    # è®¡ç®—è´¡çŒ®åº¦
    contribution = (gradient * activation).mean().item()
    
    # ğŸš€ ç¡®ä¿ `contribution` ç»Ÿä¸€å­˜åˆ° `cuda:0`
    grad_activation_scores[layer_name]["contribution"] = torch.tensor(
        contribution, device="cuda:0"
    )

# ğŸ”¹ ç»‘å®š Hooks
hooks = []
for layer in model.model.layers:
    fwd_hook = layer.register_forward_hook(forward_hook)
    bwd_hook = layer.register_full_backward_hook(backward_hook)
    hooks.extend([fwd_hook, bwd_hook])

# ğŸ”¹ è¿è¡Œæ¨¡å‹
text = "Artificial Intelligence is transforming the world with LLaMA-7B."
inputs = tokenizer(text, return_tensors="pt")
inputs = {k: v.to(model.device) for k, v in inputs.items()}  # ğŸš€ å‘é€åˆ° `model` è®¾å¤‡

# ğŸ”¹ è®¡ç®— Loss å¹¶åå‘ä¼ æ’­
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
loss.backward()

# ğŸ”¹ é‡Šæ”¾ Hooks
for hook in hooks:
    hook.remove()

# ğŸ”¹ ç»Ÿä¸€æ”¶é›† `æ¢¯åº¦ Ã— æ¿€æ´»å€¼` æ•°æ®åˆ° `cuda:0`
for layer_name, data in grad_activation_scores.items():
    if "contribution" in data:
        grad_activation_scores[layer_name]["contribution"] = data["contribution"].to("cuda:0")

# ğŸ”¹ æå–å¹¶æ’åºè´¡çŒ®åº¦
sorted_grad_activations = sorted(
    [(name, data["contribution"].item()) for name, data in grad_activation_scores.items() if "contribution" in data],
    key=lambda x: -x[1]
)

# ğŸ”¹ æ‰“å°ç»“æœ
print("\nğŸš€ **æ¢¯åº¦ Ã— æ¿€æ´»å€¼ è´¡çŒ®åº¦ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰** ğŸš€\n")
for layer, score in sorted_grad_activations:
    print(f"{layer}: Contribution={score:.6f}")
