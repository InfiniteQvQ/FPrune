import torch
from transformers import AutoModelForCausalLM, LlamaTokenizer

# âœ… è‡ªåŠ¨é€‚é…å¤š GPU
device_count = torch.cuda.device_count()
device_map = {i: f"cuda:{i}" for i in range(device_count)}
print(f"ğŸš€ Using {device_count} GPUs: {device_map}")

# âœ… åŠ è½½ LLaMA-7B
cache_dir = "/root/autodl-tmp/llm_weights"
model = AutoModelForCausalLM.from_pretrained(
    "pinkmanlove/llama-7b-hf",
    cache_dir=cache_dir,
    torch_dtype=torch.float16,
    device_map="auto",
)

tokenizer = LlamaTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")

# âœ… å­˜å‚¨æ¢¯åº¦ Ã— æ¿€æ´»å€¼
grad_activation_scores = {}

def forward_hook(layer_idx):
    """å­˜å‚¨å‰å‘ä¼ æ’­çš„æ¿€æ´»å€¼"""
    def hook(module, input, output):
        layer_name = f"LlamaDecoderLayer_{layer_idx}"  # âœ… ç›´æ¥å­˜å±‚æ•°ç´¢å¼•
        hidden_states = output[0] if isinstance(output, tuple) else output  # âœ… å…¼å®¹ tuple è¾“å‡º
        grad_activation_scores[layer_name] = {"activation": hidden_states.detach()}
    return hook

def backward_hook(layer_idx):
    """è®¡ç®—æ¢¯åº¦ Ã— æ¿€æ´»å€¼"""
    def hook(module, grad_input, grad_output):
        layer_name = f"LlamaDecoderLayer_{layer_idx}"

        gradient = grad_output[0].detach() if isinstance(grad_output, tuple) else grad_output.detach()
        activation = grad_activation_scores[layer_name]["activation"]

        # âœ… ç¡®ä¿æ¢¯åº¦å’Œæ¿€æ´»å€¼åœ¨åŒä¸€è®¾å¤‡
        if gradient.device != activation.device:
            activation = activation.to(gradient.device)

        # ğŸš€ è®¡ç®—è´¡çŒ®åº¦
        contribution = (gradient * activation).mean().item()
        grad_activation_scores[layer_name]["contribution"] = contribution

        print(f"âœ… Processed {layer_name}: Contribution={contribution:.6f}")
    return hook

# âœ… ç»‘å®š Hooks (ä¿®æ­£ä½œç”¨èŒƒå›´)
hooks = []
for idx, layer in enumerate(model.model.layers):
    hooks.append(layer.register_forward_hook(forward_hook(idx)))
    hooks.append(layer.register_full_backward_hook(backward_hook(idx)))  # âœ… å…¼å®¹ `accelerate`

# âœ… è¿è¡Œæ¨¡å‹
text = "Artificial Intelligence is transforming the world with LLaMA-7B."
inputs = tokenizer(text, return_tensors="pt")
inputs = {k: v.to(model.device) for k, v in inputs.items()}  # âœ… å‘é€åˆ° `model` è®¾å¤‡

# âœ… è®¡ç®— Loss å¹¶åå‘ä¼ æ’­
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
loss.backward()

# âœ… é‡Šæ”¾ Hooks
for hook in hooks:
    hook.remove()

# âœ… æå–å¹¶æ’åºè´¡çŒ®åº¦
sorted_grad_activations = sorted(
    [(name, data["contribution"]) for name, data in grad_activation_scores.items() if "contribution" in data],
    key=lambda x: -x[1]
)

# âœ… æ‰“å°ç»“æœ
print("\nğŸš€ **æ¢¯åº¦ Ã— æ¿€æ´»å€¼ è´¡çŒ®åº¦ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰** ğŸš€\n")
for layer, score in sorted_grad_activations:
    print(f"{layer}: Contribution={score:.6f}")
