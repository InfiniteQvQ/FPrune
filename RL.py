import argparse
import copy
import gym
import numpy as np
import torch
import torch.nn as nn
from stable_baselines3 import PPO
from transformers import AutoModelForCausalLM, LlamaTokenizer  # æˆ– AutoTokenizer
from datasets import load_dataset
from lib.layerwrapper import WrappedGPT
from lib.data import get_loaders
import warnings

# ------------------- Calibration å‡½æ•° -------------------
def prepare_calibration_input(model, dataloader, device, nsamples):
    """
    åœ¨ CPU ä¸Šåˆ›å»ºæ ¡å‡†æ•°æ®ï¼Œç„¶ååç»­æŒ‰éœ€è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡ã€‚
    """
    layers = model.model.layers
    init_device = "cpu"  # æ•°æ®å…ˆåˆ›å»ºåœ¨ CPU ä¸Š
    dtype = next(iter(model.parameters())).dtype
    inps = torch.zeros((nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=init_device)
    inps.requires_grad = False
    cache = {'i': 0, 'attention_mask': None, "position_ids": None}

    class Catcher(nn.Module):
        def __init__(self, module):
            super().__init__()
            self.module = module
        def forward(self, inp, **kwargs):
            inps[cache['i']] = inp.cpu()  # å­˜åˆ° CPU
            cache['i'] += 1
            cache['attention_mask'] = kwargs['attention_mask']
            cache['position_ids'] = kwargs.get('position_ids', None)
            raise ValueError
    layers[0] = Catcher(layers[0])
    for batch in dataloader:
        try:
            model(batch[0].to(init_device))
        except ValueError:
            pass 
    layers[0] = layers[0].module
    torch.cuda.empty_cache()
    outs = torch.zeros_like(inps)
    attention_mask = cache['attention_mask']
    position_ids = cache['position_ids']
    return inps, outs, attention_mask, position_ids

def prepare_calibration_input_opt(model, dataloader, device, nsamples):
    """
    é’ˆå¯¹ OPT æ¨¡å‹çš„æ ¡å‡†å‡½æ•°ï¼ŒåŒæ ·åœ¨ CPU ä¸Šåˆ›å»ºæ•°æ®
    """
    layers = model.model.decoder.layers
    init_device = "cpu"
    dtype = next(iter(model.parameters())).dtype
    inps = torch.zeros((nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=init_device)
    inps.requires_grad = False
    cache = {'i': 0, 'attention_mask': None}
    
    class Catcher(nn.Module):
        def __init__(self, module):
            super().__init__()
            self.module = module
        def forward(self, inp, **kwargs):
            inps[cache['i']] = inp.cpu()
            cache['i'] += 1
            cache['attention_mask'] = kwargs['attention_mask']
            raise ValueError
        
    layers[0] = Catcher(layers[0])
    for batch in dataloader:
        try:
            model(batch[0].to(init_device))
        except ValueError:
            pass 
    layers[0] = layers[0].module
    torch.cuda.empty_cache()
    outs = torch.zeros_like(inps)
    attention_mask = cache['attention_mask']
    return inps, outs, attention_mask, None

# ------------------- å·¥å…·å‡½æ•° -------------------
def find_layers(module, layers=[nn.Linear], name=''):
    """
    é€’å½’æŸ¥æ‰¾æ¨¡å—ä¸­æŒ‡å®šç±»å‹çš„å±‚
    """
    if type(module) in layers:
        return {name: module}
    res = {}
    for name1, child in module.named_children():
        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))
    return res

def return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before):
    thres_cumsum = sum_before * alpha 
    sort_mask = tmp_metric <= thres_cumsum.reshape((-1,1))
    thres = torch.gather(sort_res[0], dim=1, index=sort_mask.sum(dim=1, keepdims=True)-1)
    W_mask = (W_metric <= thres)
    cur_sparsity = (W_mask==True).sum() / W_mask.numel()
    return W_mask, cur_sparsity

# ------------------- å‰ªæå‡½æ•°ï¼ˆWandaï¼‰ -------------------
def prune_wanda_ww(args, model, tokenizer, device=torch.device("cuda:0"), prune_n=0, prune_m=0, prune_ratios=0):
    """
    æ ¹æ®ä¼ å…¥çš„æ¯å±‚å‰ªææ¯”ä¾‹ï¼Œè°ƒç”¨ wanda å‰ªæ
    """
    s1 = 1.0 - args.epsilon
    s2 = 1.0 + args.epsilon
    res = []
    for j in prune_ratios:
        for i in range(7):  # å‡è®¾æ¯ä¸ª transformer å±‚å†…éƒ¨æœ‰ 7 ä¸ªå­å±‚
            res.append(j)
    res = np.array(res)
    prune_wanda(args, model, tokenizer, device, ratios=res) 

def prune_wanda(args, model, tokenizer, device=torch.device("cuda:0"), prune_n=0, prune_m=0, ratios=None):
    use_cache = model.config.use_cache 
    model.config.use_cache = False 

    print("loading calibration data")
    dataloader, _ = get_loaders("c4", nsamples=args.nsamples, seed=args.seed, seqlen=model.seqlen, tokenizer=tokenizer)
    print("dataset loading complete")
    with torch.no_grad():
        if "OPT" in model.__class__.__name__:
            inps, outs, attention_mask, position_ids = prepare_calibration_input_opt(model, dataloader, device, args.nsamples)
        else:
            inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device, args.nsamples)
    print("inps", inps.shape)
    if "OPT" in model.__class__.__name__:
        layers = model.model.decoder.layers
    else:    
        layers = model.model.layers
    layer_num = len(find_layers(layers))
    if ratios is None:
        ratios = [args.sparsity_ratio for _ in range(layer_num)]
    
    k = 0
    for i in range(len(layers)):
        layer = layers[i]
        subset = find_layers(layer)
        # å¦‚æœè¯¥å±‚æœ‰å¤š GPU åˆ†é…ï¼Œåˆ™å–å½“å‰å±‚å¯¹åº”è®¾å¤‡ï¼ˆä¸ä¸€æ¬¡æ€§è½¬ç§»æ•´ä¸ª inpsï¼‰
        if f"model.layers.{i}" in model.hf_device_map:
            dev = model.hf_device_map[f"model.layers.{i}"]
            print(f"using wanda! layer {i} device {dev}")
        else:
            dev = device
        wrapped_layers = {}
        for name in subset:
            wrapped_layers[name] = WrappedGPT(subset[name])
        def add_batch(name):
            def tmp(_, inp, out):
                wrapped_layers[name].add_batch(inp[0].data, out.data)
            return tmp
        handles = []
        for name in wrapped_layers:
            handles.append(subset[name].register_forward_hook(add_batch(name)))
        # å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬è½¬ç§»åˆ°å½“å‰å±‚è®¾å¤‡è¿›è¡Œå‰å‘ä¼ æ’­
        for j in range(args.nsamples):
            with torch.no_grad():
                sample_inp = inps[j].unsqueeze(0).to(dev)
                sample_attention = attention_mask.to(dev)
                if position_ids is not None:
                    sample_position_ids = position_ids.to(dev)
                else:
                    sample_position_ids = None
                if "OPT" in model.__class__.__name__:
                    outs[j] = layer(sample_inp, attention_mask=sample_attention)[0]
                else:
                    outs[j] = layer(sample_inp, attention_mask=sample_attention, position_ids=sample_position_ids)[0]
        for h in handles:
            h.remove()
        for name in subset:
            print(f"pruning layer {i} name {name}")
            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1, -1)))
            W_mask = (torch.zeros_like(W_metric) == 1)
            if prune_n != 0:
                for ii in range(W_metric.shape[1]):
                    if ii % prune_m == 0:
                        tmp = W_metric[:, ii:(ii+prune_m)].float()
                        W_mask.scatter_(1, ii + torch.topk(tmp, prune_n, dim=1, largest=False)[1], True)
            else:
                sort_res = torch.sort(W_metric, dim=-1, stable=True)
                if args.use_variant:
                    tmp_metric = torch.cumsum(sort_res[0], dim=1)
                    sum_before = W_metric.sum(dim=1)
                    alpha = 0.4
                    alpha_hist = [0., 0.8]
                    W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)
                    while (torch.abs(cur_sparsity - args.sparsity_ratio) > 0.001) and (alpha_hist[1]-alpha_hist[0] >= 0.001):
                        if cur_sparsity > args.sparsity_ratio:
                            alpha_new = (alpha + alpha_hist[0]) / 2.0
                            alpha_hist[1] = alpha
                        else:
                            alpha_new = (alpha + alpha_hist[1]) / 2.0
                            alpha_hist[0] = alpha
                        alpha = alpha_new 
                        W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)
                    print(f"alpha found {alpha} sparsity {cur_sparsity:.6f}")
                else:
                    indices = sort_res[1][:, :int(W_metric.shape[1]*ratios[k])]
                    k += 1
                    W_mask.scatter_(1, indices, True)
            subset[name].weight.data[W_mask] = 0
        # æ¯ä¸ªæ ·æœ¬å‰å‘è®¡ç®—ï¼Œå•ç‹¬è½¬ç§»åˆ°å½“å‰è®¾å¤‡
        for j in range(args.nsamples):
            with torch.no_grad():
                sample_inp = inps[j].unsqueeze(0).to(dev)
                sample_attention = attention_mask.to(dev)
                if position_ids is not None:
                    sample_position_ids = position_ids.to(dev)
                else:
                    sample_position_ids = None
                if "OPT" in model.__class__.__name__:
                    outs[j] = layer(sample_inp, attention_mask=sample_attention)[0]
                else:
                    outs[j] = layer(sample_inp, attention_mask=sample_attention, position_ids=sample_position_ids)[0]
        # å°†æœ¬è½®å‰å‘ç»“æœç§»å› CPUï¼Œä¾›ä¸‹ä¸€å±‚ä½¿ç”¨
        inps = outs.cpu()
    model.config.use_cache = use_cache 
    torch.cuda.empty_cache()

# ------------------- RL ç¯å¢ƒ -------------------
class PruningEnv(gym.Env):
    """
    RL ç¯å¢ƒï¼š
      - æ¯ä¸ª episode ä»åŸå§‹æ¨¡å‹çŠ¶æ€å¼€å§‹ï¼ˆé€šè¿‡ load_state_dict æ¢å¤ï¼‰ã€‚
      - åŠ¨ä½œä¸ºæ¯å±‚çš„å‰ªææ¯”ä¾‹æƒé‡ï¼ˆESD éƒ¨åˆ†ä¸ GradNorm éƒ¨åˆ†ç»„åˆï¼‰ã€‚
      - ç¯å¢ƒæ ¹æ®åŠ¨ä½œå¯¹æ¨¡å‹è¿›è¡Œå‰ªæï¼Œç„¶ååœ¨éªŒè¯é›†ä¸Šè®¡ç®— perplexityï¼ˆæˆ– lossï¼‰ä½œä¸ºè¯„ä»·ï¼Œè¿”å› rewardã€‚
    """
    def __init__(self, model, esd_ratios, importance_scores, args, tokenizer, device, inputs, base_loss):
        super(PruningEnv, self).__init__()
        self.model = model
        self.esd_ratios = esd_ratios
        self.importance_scores = importance_scores
        self.num_layers = len(esd_ratios)
        self.args = args
        self.tokenizer = tokenizer
        self.device = device
        self.inputs = inputs
        self.base_loss = base_loss
        # ä¿å­˜åŸå§‹æ¨¡å‹çŠ¶æ€ï¼ˆåœ¨ CPU ä¸Šä¿å­˜ï¼Œä¿è¯æ¯ä¸ª episode ä»å¹²å‡€çŠ¶æ€å¼€å§‹ï¼‰
        self.initial_state = copy.deepcopy({k: v.cpu() for k, v in model.state_dict().items()})
        # åŠ¨ä½œç©ºé—´ï¼šæ¯å±‚çš„ ESD æƒé‡ï¼Œå–å€¼èŒƒå›´ [0,1]
        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.num_layers,), dtype=np.float32)
        # è§‚å¯Ÿç©ºé—´ï¼šç®€å•è¿”å›ä¸¤ç»„å‰ªææ¯”ä¾‹ï¼ˆESD ä¸ GradNormï¼‰ï¼Œè¿™é‡Œå›ºå®šä¸å˜ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•
        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.num_layers*2,), dtype=np.float32)

    def reset(self):
        # æ¯ä¸ª episode å¼€å§‹æ—¶ï¼Œæ¢å¤åŸå§‹æ¨¡å‹çŠ¶æ€
        self.model.load_state_dict(self.initial_state)
        self.esd_weights = np.ones(self.num_layers) * 0.8  # åˆå§‹é»˜è®¤åŠ¨ä½œ
        return np.concatenate([self.esd_ratios, self.importance_scores])

    def step(self, action):
        # é™å®šåŠ¨ä½œåœ¨ [0,1] å†…
        self.esd_weights = np.clip(action, 0.0, 1.0)
        # è®¡ç®—æœ€ç»ˆå‰ªææ¯”ä¾‹ï¼ˆè¿™é‡Œå‡è®¾æœ€ç»ˆå‰ªææ¯”ä¾‹ç”± ESD éƒ¨åˆ†å’Œ GradNorm éƒ¨åˆ†çº¿æ€§ç»„åˆå¾—åˆ°ï¼‰
        final_pruning_ratios = self.esd_weights * self.esd_ratios + (1 - self.esd_weights) * self.importance_scores
        # å¯¹æ¨¡å‹è¿›è¡Œå‰ªæï¼ˆè°ƒç”¨ wanda å‰ªæï¼‰
        prune_wanda_ww(self.args, self.model, self.tokenizer, self.device, prune_ratios=final_pruning_ratios)
        # è¯„ä¼°å‰ªæåçš„æ¨¡å‹ï¼ˆè®¡ç®— loss æˆ– perplexityï¼‰
        with torch.no_grad():
            outputs = self.model(**self.inputs, labels=self.inputs["input_ids"])
            pruned_loss = outputs.loss.item()
        # å¥–åŠ±è®¾è®¡ï¼šä»¥å‰ªæå loss ç›¸å¯¹äºåŸå§‹ loss çš„å˜åŒ–ä½œä¸ºå¥–åŠ±ï¼ˆloss è¶Šä½ï¼Œreward è¶Šé«˜ï¼‰
        loss_increase = (pruned_loss - self.base_loss) / self.base_loss
        reward = -loss_increase
        done = True  # æ¯ä¸ª episode åªæœ‰ä¸€æ¬¡å‰ªæè¯„ä¼°
        obs = np.concatenate([self.esd_ratios, self.importance_scores])
        return obs, reward, done, {}

# ------------------- ä¸»ç¨‹åº -------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default="pinkmanlove/llama-7b-hf", help="LLaMA æ¨¡å‹è·¯å¾„")
    parser.add_argument('--cache_dir', type=str, default="/root/autodl-tmp/llm_weights", help="æ¨¡å‹ç¼“å­˜è·¯å¾„")
    parser.add_argument('--ww_metric', type=str, default="alpha_peak", help="ESD å‰ªææŒ‡æ ‡")
    parser.add_argument('--ww_metric_cache', type=str, default="./data/llama-7b-hf", help="ESD æŒ‡æ ‡å­˜å‚¨è·¯å¾„")
    parser.add_argument('--sparsity_ratio', type=float, default=0.7, help="ç›®æ ‡å‰ªææ¯”ä¾‹")
    parser.add_argument('--prune_method', type=str, default="wanda_ww", help="å‰ªææ–¹æ³•")
    parser.add_argument('--epsilon', type=float, default=0.2, help="å‰ªææ¯”ä¾‹çš„å¾®è°ƒèŒƒå›´")
    parser.add_argument('--nsamples', type=int, default=10, help="æ ¡å‡†æ ·æœ¬æ•°")
    parser.add_argument('--seed', type=int, default=42, help="éšæœºç§å­")
    parser.add_argument('--use_variant', action='store_true', help="æ˜¯å¦ä½¿ç”¨ Wanda variant å‰ªæ")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ device_map="auto" è‡ªåŠ¨åˆ†é…å„å±‚åˆ°å¤šä¸ª GPUï¼‰
    model = AutoModelForCausalLM.from_pretrained(args.model, cache_dir=args.cache_dir, device_map="auto", torch_dtype=torch.float16)
    tokenizer_name = "HuggingFaceM4/llama-7b-tokenizer"
    tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name)

    # å¦‚æœæ¨¡å‹æ²¡æœ‰ seqlen å±æ€§ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„ max_position_embeddings
    if not hasattr(model, 'seqlen'):
        model.seqlen = model.config.max_position_embeddings

    # åŠ è½½æ•°æ®é›†ï¼Œå¹¶å¯¹éƒ¨åˆ†æ–‡æœ¬è¿›è¡Œ tokenization
    dataset = load_dataset("roneneldan/TinyStories", split="train")
    sample_texts = [dataset[i]["text"] for i in range(100)]
    inputs = tokenizer(sample_texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # è®¡ç®—åŸå§‹æ¨¡å‹çš„ lossï¼ˆä½œä¸º baselineï¼‰ï¼Œè¿™é‡Œ loss è¶Šä½è¡¨ç¤ºæ¨¡å‹æ•ˆæœè¶Šå¥½
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        base_loss = outputs.loss.item()
    print(f"ğŸš€ å‰ªæå‰ LLaMA-7B åœ¨ TinyStories Loss: {base_loss:.6f}")

    # ç¤ºä¾‹çš„ ESD å‰ªææ¯”ä¾‹ä¸ GradNormï¼ˆæˆ–å…¶å®ƒé‡è¦æ€§æŒ‡æ ‡ï¼‰ï¼Œè¿™ä¸¤ä¸ªæ•°ç»„åº”å½“é¢„å…ˆè®¡ç®—å¥½
    esd_ratios = np.array([
        0.57042164, 0.61759788, 0.63153112, 0.63073802, 0.65285629, 0.6482451,
        0.63005912, 0.5921672,  0.59738964, 0.56803465, 0.58708227, 0.58937198,
        0.59899241, 0.61086321, 0.61877495, 0.66812801, 0.65868002, 0.71560568,
        0.79057246, 0.74378908, 0.79461485, 0.82483709, 0.77005184, 0.76292461,
        0.81216604, 0.85205203, 0.8312614,  0.84147072, 0.78692085, 0.82967305,
        0.84142309, 0.73170304
    ])
    importance_scores = np.array([
        0, 0.25823024, 0.64031047, 0.672687, 0.7274453, 0.7274453,
        0.7274453, 0.7274453, 0.7274453, 0.7274453, 0.7274453, 0.7274453,
        0.7462126, 0.7462126, 0.7462126, 0.74832606, 0.74832606, 0.74832606,
        0.74936193, 0.74936193, 0.74936193, 0.74992037, 0.74992037, 0.74992037,
        0.75013256, 0.75013256, 0.75013256, 0.75013256, 0.75394976, 0.75394976,
        0.7545204,  0.764797
    ])

    # åˆ›å»º RL ç¯å¢ƒï¼Œæ¯ä¸ª episode éƒ½å°†é‡æ–°åŠ è½½åŸå§‹æ¨¡å‹çŠ¶æ€
    env = PruningEnv(model, esd_ratios, importance_scores, args, tokenizer, device, inputs, base_loss)
    # PPO ä½¿ç”¨ MlpPolicyï¼Œæ­¤å¤„ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œåç»­å¯æ ¹æ®éœ€è¦è°ƒæ•´
    model_rl = PPO("MlpPolicy", env, verbose=1)
    model_rl.learn(total_timesteps=5000)

    # è®­ç»ƒç»“æŸåï¼Œåˆ©ç”¨ RL ä»£ç†å¾—åˆ°æœ€ä¼˜å‰ªææ¯”ä¾‹ï¼ˆæ¯å±‚çš„ ESD æƒé‡ï¼‰
    best_action = model_rl.predict(env.reset())[0]
    print(f"ğŸš€ æœ€ä¼˜ ESD æƒé‡ï¼ˆæ¯å±‚ï¼‰: {best_action}")
    final_pruning_ratios = best_action * esd_ratios + (1 - best_action) * importance_scores
    print("ğŸ”¥ RL è®¡ç®—çš„æœ€ç»ˆå‰ªææ¯”ä¾‹:", final_pruning_ratios)
    np.save("final_pruning_ratios_rl.npy", final_pruning_ratios)
