import torch
import torch.nn.functional as F
import numpy as np
from transformers import LlamaModel, AutoTokenizer

# **ğŸ”¥ ç¡®ä¿æ¨¡å‹ç”¨å¤šä¸ª GPU**
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "pinkmanlove/llama-7b-hf"

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")
model = LlamaModel.from_pretrained(
    model_name,
    cache_dir="/root/autodl-tmp/llm_weights",
    output_hidden_states=True,
    torch_dtype=torch.float16,  
    device_map="auto"  # **è‡ªåŠ¨åˆ†é…å¤šä¸ª GPU**
)

# **è¾“å…¥å¤„ç†**
text = ["LLaMA 7B Kernel Target Alignment computation."]
inputs = tokenizer(text, return_tensors="pt")
inputs.pop("token_type_ids", None)
inputs = {key: val.to(device) for key, val in inputs.items()}

# **å‰å‘ä¼ æ’­**
with torch.no_grad():
    outputs = model(**inputs)

hidden_states = outputs.hidden_states  # tuple of (num_layers, batch, seq_len, hidden_dim)
num_layers = len(hidden_states) - 1  # ä¸åŒ…æ‹¬ embedding å±‚

# **ğŸ”¥ é€å—è®¡ç®— Wasserstein è·ç¦»ï¼Œé˜²æ­¢æ˜¾å­˜æº¢å‡º**
def wasserstein_distance_torch(H1, H2, eps=1e-3, max_iter=50, chunk_size=512):
    """
    è®¡ç®— Sinkhorn-Wasserstein è·ç¦»ï¼Œé€å—è®¡ç®— Cost çŸ©é˜µï¼Œå‡å°‘æ˜¾å­˜å ç”¨
    """
    H1 = H1.to(torch.float16)  # **é™ä½ç²¾åº¦å‡å°‘æ˜¾å­˜å ç”¨**
    H2 = H2.to(torch.float16)  

    n, d = H1.shape
    m, _ = H2.shape

    # **åˆå§‹åŒ– cost_matrix**
    cost_matrix = torch.zeros(n, m, dtype=torch.float16, device=H1.device)

    # ğŸ”¥ **åˆ†å—è®¡ç®— Cost çŸ©é˜µ**
    for i in range(0, n, chunk_size):
        for j in range(0, m, chunk_size):
            sub_H1 = H1[i : i + chunk_size]
            sub_H2 = H2[j : j + chunk_size]
            cost_matrix[i : i + chunk_size, j : j + chunk_size] = torch.cdist(sub_H1, sub_H2, p=2).pow(2)

    torch.cuda.empty_cache()  # **é‡Šæ”¾æ˜¾å­˜**
    
    # **åˆå§‹åŒ–åˆ†å¸ƒ**
    a = torch.ones(n, device=H1.device) / n
    b = torch.ones(m, device=H2.device) / m

    u = torch.zeros_like(a)
    v = torch.zeros_like(b)

    # **ğŸ”¥ Sinkhorn-Knopp è¿­ä»£**
    for _ in range(max_iter):
        u = -torch.logsumexp((-cost_matrix + v[None, :]) / eps, dim=1) + torch.log(a)
        v = -torch.logsumexp((-cost_matrix + u[:, None]) / eps, dim=0) + torch.log(b)

    return (u[:, None] + v[None, :] - cost_matrix).exp().sum()

# **ğŸ”¥ è®¡ç®— Wasserstein**
layerwise_wasserstein = []

for layer_idx in range(num_layers - 1):
    H = hidden_states[layer_idx][0].to(torch.float16)
    H_next = hidden_states[layer_idx + 1][0].to(torch.float16)

    # **ğŸ”¥ ä½¿ç”¨ Chunk è®¡ç®— Wassersteinï¼Œé¿å… OOM**
    wd = wasserstein_distance_torch(H, H_next, chunk_size=256)  
    layerwise_wasserstein.append(wd.item())

# **ğŸ”¥ å½’ä¸€åŒ– Wasserstein**
wasserstein_scores = torch.tensor(layerwise_wasserstein, device="cuda")
wasserstein_scores = (wasserstein_scores - wasserstein_scores.min()) / (wasserstein_scores.max() - wasserstein_scores.min())

# **ğŸ”¥ è¾“å‡º**
print("âœ… Wasserstein è®¡ç®—å®Œæˆ")
print("Wasserstein Scores:", wasserstein_scores.cpu().numpy())
