import torch
import numpy as np
from transformers import LlamaModel, AutoTokenizer

# ğŸš€ è®¡ç®— CKA ç›¸ä¼¼æ€§
def cka_similarity(X, Y):
    X, Y = X.to(torch.float32), Y.to(torch.float32)  
    K_X = X @ X.transpose(-1, -2)  
    K_Y = Y @ Y.transpose(-1, -2)
    num = (K_X * K_Y).sum()
    denom = torch.sqrt((K_X * K_X).sum()) * torch.sqrt((K_Y * K_Y).sum())
    return num / (denom + 1e-6)

# ğŸš€ åŠ è½½ LLaMA 7B
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "pinkmanlove/llama-7b-hf"

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")
model = LlamaModel.from_pretrained(
    model_name,
    cache_dir="/root/autodl-tmp/llm_weights",
    output_hidden_states=True,
    torch_dtype=torch.float16,  
    device_map="auto"
)

# ğŸš€ å¤„ç†è¾“å…¥
text = ["LLaMA 7B Kernel Target Alignment computation."]
inputs = tokenizer(text, return_tensors="pt")
inputs.pop("token_type_ids", None)
inputs = {key: val.to(device) for key, val in inputs.items()}

# ğŸš€ è®¡ç®—éšè—çŠ¶æ€
with torch.no_grad():
    outputs = model(**inputs)

hidden_states = outputs.hidden_states  # (num_layers, batch, seq_len, hidden_dim)

# ğŸš€ **ç¡®ä¿åªå– 32 å±‚ Transformer**
hidden_states = hidden_states[1:]  # **å»æ‰ç¬¬ 0 å±‚ï¼ˆè¾“å…¥åµŒå…¥å±‚ï¼‰**
num_layers = len(hidden_states)  # ğŸš€ ç°åœ¨ num_layers åº”è¯¥æ˜¯ 32

# ğŸš€ è®¡ç®— 32 å±‚çš„ CKA ç›¸ä¼¼åº¦
cka_matrix = torch.zeros(num_layers, num_layers).to(device)

for i in range(num_layers):
    for j in range(i, num_layers):  
        cka_matrix[i, j] = cka_similarity(hidden_states[i][0], hidden_states[j][0])
        cka_matrix[j, i] = cka_matrix[i, j]  # å¯¹ç§°çŸ©é˜µ

cka_importance = cka_matrix.mean(dim=1)  # è®¡ç®—æ¯å±‚çš„å¹³å‡ CKA ç›¸ä¼¼æ€§

# ğŸš€ ESD å‰ªææ¯”ä¾‹
esd_pruning_ratios = torch.tensor([
    0.5704, 0.6176, 0.6315, 0.6307, 0.6528, 0.6482, 0.6300, 0.5921, 0.5973, 0.5680,
    0.5870, 0.5893, 0.5989, 0.6108, 0.6187, 0.6681, 0.6586, 0.7156, 0.7905, 0.7437,
    0.7946, 0.8248, 0.7700, 0.7629, 0.8121, 0.8520, 0.8312, 0.8414, 0.7869, 0.8296,
    0.8414, 0.7317
]).to(device)

# **ğŸš€ ç¡®ä¿ ESD ç»´åº¦åŒ¹é…**
if esd_pruning_ratios.shape[0] != num_layers:
    esd_pruning_ratios = esd_pruning_ratios[:num_layers]  # **ç¡®ä¿ ESD ä¹Ÿæ˜¯ 32 å±‚**

# **ğŸš€ åè½¬ CKAï¼Œç¡®ä¿é«˜ CKA é‡è¦æ€§ä½**
cka_importance = 1 - cka_importance  

# ğŸš€ å½’ä¸€åŒ– CKA åˆ° 0~1
cka_importance = (cka_importance - cka_importance.min()) / (cka_importance.max() - cka_importance.min())

# **æœ€ç»ˆå‰ªææ¯”ä¾‹ï¼ˆè®©é«˜é‡è¦æ€§å±‚å‰ªææ›´å°‘ï¼‰**
adjusted_pruning_ratios = esd_pruning_ratios * (1 - 0.5 * cka_importance)

# **ğŸš€ å½’ä¸€åŒ–ï¼Œä¿æŒå‰ªææ¯”ä¾‹å‡å€¼ä¸å˜**
original_mean = esd_pruning_ratios.mean()
adjusted_mean = adjusted_pruning_ratios.mean()
final_pruning_ratios = adjusted_pruning_ratios * (original_mean / adjusted_mean)

print("Final Adjusted Pruning Ratios:", final_pruning_ratios.cpu().numpy())
print(final_pruning_ratios.mean())
a = []
for i in final_pruning_ratios.cpu().numpy():
    a.append(i)
print(a)
