import torch
import torch.nn.functional as F
import numpy as np
from transformers import LlamaModel, AutoTokenizer

# **ğŸ”¥ ç¡®ä¿æ¨¡å‹ç”¨å¤šä¸ª GPU**
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "pinkmanlove/llama-7b-hf"

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceM4/llama-7b-tokenizer")
model = LlamaModel.from_pretrained(
    model_name,
    cache_dir="/root/autodl-tmp/llm_weights",
    output_hidden_states=True,
    torch_dtype=torch.float16,  
    device_map="auto"  # **è‡ªåŠ¨åˆ†é…å¤šä¸ª GPU**
)

# **è¾“å…¥å¤„ç†**
text = ["LLaMA 7B Kernel Target Alignment computation."]
inputs = tokenizer(text, return_tensors="pt")
inputs.pop("token_type_ids", None)
inputs = {key: val.to(device) for key, val in inputs.items()}

# **å‰å‘ä¼ æ’­**
with torch.no_grad():
    outputs = model(**inputs)

hidden_states = outputs.hidden_states  # tuple of (num_layers, batch, seq_len, hidden_dim)
num_layers = len(hidden_states) - 1  # ä¸åŒ…æ‹¬ embedding å±‚

# **ğŸ”¥ Mini-Batch Wasserstein è·ç¦»è®¡ç®—**
def wasserstein_distance_torch(H1, H2, eps=1e-3, max_iter=50, batch_size=1024):
    """
    è®¡ç®— Sinkhorn-Wasserstein è·ç¦»ï¼Œæ”¯æŒ mini-batch è®¡ç®—ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
    """
    H1 = H1.view(-1, 1).float()
    H2 = H2.view(-1, 1).float()

    n = H1.size(0)
    m = H2.size(0)

    # ğŸ”¥ **åˆ†æ‰¹è®¡ç®— Cost çŸ©é˜µ**
    cost_matrix = torch.zeros(n, m, device=H1.device)
    
    for i in range(0, n, batch_size):
        for j in range(0, m, batch_size):
            sub_H1 = H1[i : i + batch_size]
            sub_H2 = H2[j : j + batch_size]
            cost_matrix[i : i + batch_size, j : j + batch_size] = torch.cdist(sub_H1, sub_H2, p=2).pow(2)

    # **åˆå§‹åŒ–åˆ†å¸ƒ**
    a = torch.ones(n, device=H1.device) / n
    b = torch.ones(m, device=H2.device) / m

    u = torch.zeros_like(a)
    v = torch.zeros_like(b)

    # **ğŸ”¥ Sinkhorn-Knopp è¿­ä»£**
    for _ in range(max_iter):
        u = -torch.logsumexp((-cost_matrix + v[None, :]) / eps, dim=1) + torch.log(a)
        v = -torch.logsumexp((-cost_matrix + u[:, None]) / eps, dim=0) + torch.log(b)

    return (u[:, None] + v[None, :] - cost_matrix).exp().sum()


# **ğŸ”¥ Mini-Batch äº’ä¿¡æ¯è®¡ç®—**
def mutual_information_torch(H1, H2, num_bins=256, batch_size=1024):
    """
    è®¡ç®—äº’ä¿¡æ¯ï¼Œæ”¯æŒ mini-batch è®¡ç®—ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
    """
    H1 = H1.view(-1).float()
    H2 = H2.view(-1).float()

    # ğŸ”¥ **æŒ‰ batch è®¡ç®—ç›´æ–¹å›¾**
    hist2d = torch.zeros(num_bins, num_bins, device=H1.device)

    for i in range(0, H1.numel(), batch_size):
        sub_H1 = H1[i : i + batch_size]
        sub_H2 = H2[i : i + batch_size]
        hist = torch.histogram2d(sub_H1, sub_H2, bins=num_bins, weight=None, range=None)
        hist2d += hist.hist

    # å½’ä¸€åŒ–
    P_xy = hist2d / torch.sum(hist2d)
    P_x = torch.sum(P_xy, dim=1, keepdim=True)
    P_y = torch.sum(P_xy, dim=0, keepdim=True)

    # è®¡ç®—äº’ä¿¡æ¯
    P_xy_nonzero = P_xy[P_xy > 0]
    P_x_nonzero = P_x[P_x > 0]
    P_y_nonzero = P_y[P_y > 0]

    I_xy = (P_xy_nonzero * torch.log(P_xy_nonzero / (P_x_nonzero * P_y_nonzero))).sum()
    return I_xy


# **ğŸ”¥ è®¡ç®— Wasserstein å’Œ äº’ä¿¡æ¯**
layerwise_wasserstein = []
layerwise_mutual_info = []

for layer_idx in range(num_layers - 1):
    H = hidden_states[layer_idx][0].to(torch.float32)
    H_next = hidden_states[layer_idx + 1][0].to(torch.float32)

    # ğŸ”¥ **ä½¿ç”¨ Mini-Batch è®¡ç®— Wasserstein**
    wd = wasserstein_distance_torch(H, H_next, batch_size=512)  # **å°æ‰¹é‡è®¡ç®—**
    layerwise_wasserstein.append(wd.item())

    # ğŸ”¥ **ä½¿ç”¨ Mini-Batch è®¡ç®—äº’ä¿¡æ¯**
    mi = mutual_information_torch(H, H_next, batch_size=512)
    layerwise_mutual_info.append(mi.item())

# **å½’ä¸€åŒ– Wasserstein & äº’ä¿¡æ¯**
wasserstein_scores = torch.tensor(layerwise_wasserstein, device="cuda")
mutual_info_scores = torch.tensor(layerwise_mutual_info, device="cuda")
wasserstein_scores = (wasserstein_scores - wasserstein_scores.min()) / (wasserstein_scores.max() - wasserstein_scores.min())
mutual_info_scores = (mutual_info_scores - mutual_info_scores.min()) / (mutual_info_scores.max() - mutual_info_scores.min())

# **å·²æœ‰çš„ ESD å‰ªææ¯”ä¾‹**
esd_pruning_ratios = torch.tensor([
    0.5704, 0.6176, 0.6315, 0.6307, 0.6528, 0.6482, 0.6300, 0.5921,
    0.5973, 0.5680, 0.5870, 0.5893, 0.5989, 0.6108, 0.6187, 0.6681,
    0.6586, 0.7156, 0.7905, 0.7437, 0.7946, 0.8248, 0.7700, 0.7629,
    0.8121, 0.8520, 0.8312, 0.8414, 0.7869, 0.8296, 0.8414, 0.7317
], device="cuda")

# **ğŸ”¥ èåˆ ESD é‡è¦æ€§ & å±‚é—´å…³ç³»**
alpha = 0.5  # Wasserstein å½±å“æƒé‡
beta = 0.5   # äº’ä¿¡æ¯å½±å“æƒé‡

adjusted_pruning_ratios = esd_pruning_ratios.clone()
for i in range(num_layers - 1):
    adjusted_pruning_ratios[i] *= (1 + alpha * wasserstein_scores[i] - beta * mutual_info_scores[i])

# **å½’ä¸€åŒ–**
adjusted_pruning_ratios = adjusted_pruning_ratios / torch.sum(adjusted_pruning_ratios) * torch.sum(esd_pruning_ratios)

# **è¾“å‡º**
print("âœ… åŸå§‹å‰ªææ¯”ä¾‹:", esd_pruning_ratios.cpu().numpy())
print("âœ… è°ƒæ•´åå‰ªææ¯”ä¾‹:", adjusted_pruning_ratios.cpu().numpy())
