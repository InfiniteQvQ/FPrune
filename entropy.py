import torch
import numpy as np
from transformers import AutoModelForCausalLM

# **è·å– GPU è®¾å¤‡**
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ¯ è®¡ç®— Spectral Entropyï¼ˆè¡¡é‡ä¿¡æ¯å¯†åº¦ï¼‰
def compute_spectral_entropy(weight_matrix):
    eigs = torch.linalg.svdvals(weight_matrix).pow(2)
    eigs = eigs / eigs.sum()
    entropy = -torch.sum(eigs * torch.log(eigs + 1e-9))
    return entropy.item()

# ğŸ¯ è®¡ç®— Spectral Normï¼ˆè¡¡é‡æƒé‡çŸ©é˜µçš„é‡è¦æ€§ï¼‰
def compute_spectral_norm(weight_matrix):
    return torch.linalg.svdvals(weight_matrix).max().item()

# ğŸ¯ è®¡ç®— Gradient Normï¼ˆè¡¡é‡æ¢¯åº¦å¯¹æ¨¡å‹çš„å½±å“ï¼‰
def compute_gradient_norm(weight_matrix):
    return torch.norm(weight_matrix.grad).item() if weight_matrix.grad is not None else 0.0

# ğŸš€ è®¡ç®—æ¯ä¸ªæ¨¡å—çš„ Entropy / Norm / Gradient Norm
def analyze_layer(layer):
    results = {}

    # **ç¡®ä¿æƒé‡çŸ©é˜µåœ¨ GPU ä¸Šè®¡ç®—**
    results["QKV_Entropy"] = (
        compute_spectral_entropy(layer.self_attn.q_proj.weight.to(device)) +
        compute_spectral_entropy(layer.self_attn.k_proj.weight.to(device)) +
        compute_spectral_entropy(layer.self_attn.v_proj.weight.to(device))
    )
    results["QKV_Norm"] = (
        compute_spectral_norm(layer.self_attn.q_proj.weight.to(device)) +
        compute_spectral_norm(layer.self_attn.k_proj.weight.to(device)) +
        compute_spectral_norm(layer.self_attn.v_proj.weight.to(device))
    )
    results["QKV_Grad"] = (
        compute_gradient_norm(layer.self_attn.q_proj.weight) +
        compute_gradient_norm(layer.self_attn.k_proj.weight) +
        compute_gradient_norm(layer.self_attn.v_proj.weight)
    )

    results["Output_Entropy"] = compute_spectral_entropy(layer.self_attn.o_proj.weight.to(device))
    results["Output_Norm"] = compute_spectral_norm(layer.self_attn.o_proj.weight.to(device))
    results["Output_Grad"] = compute_gradient_norm(layer.self_attn.o_proj.weight)

    results["Gate_Entropy"] = compute_spectral_entropy(layer.mlp.gate_proj.weight.to(device))
    results["Gate_Norm"] = compute_spectral_norm(layer.mlp.gate_proj.weight.to(device))
    results["Gate_Grad"] = compute_gradient_norm(layer.mlp.gate_proj.weight)

    results["Up_Entropy"] = compute_spectral_entropy(layer.mlp.up_proj.weight.to(device))
    results["Up_Norm"] = compute_spectral_norm(layer.mlp.up_proj.weight.to(device))
    results["Up_Grad"] = compute_gradient_norm(layer.mlp.up_proj.weight)

    results["Down_Entropy"] = compute_spectral_entropy(layer.mlp.down_proj.weight.to(device))
    results["Down_Norm"] = compute_spectral_norm(layer.mlp.down_proj.weight.to(device))
    results["Down_Grad"] = compute_gradient_norm(layer.mlp.down_proj.weight)

    return results

# ğŸš€ å¤„ç†æ•´ä¸ªæ¨¡å‹
def process_model(model_name, model_path):
    print(f"\nğŸš€ Loading {model_name} on {device}...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        cache_dir="/root/autodl-tmp/llm_weights",
        device_map="auto",
        torch_dtype=torch.float16
    ).to(device)  # **åŠ è½½æ¨¡å‹åˆ° GPU**

    all_results = {
        "QKV_Entropy": [], "QKV_Norm": [], "QKV_Grad": [],
        "Output_Entropy": [], "Output_Norm": [], "Output_Grad": [],
        "Gate_Entropy": [], "Gate_Norm": [], "Gate_Grad": [],
        "Up_Entropy": [], "Up_Norm": [], "Up_Grad": [],
        "Down_Entropy": [], "Down_Norm": [], "Down_Grad": []
    }
    
    for idx, layer in enumerate(model.model.layers):
        print(f"Processing Layer {idx}...")
        layer_results = analyze_layer(layer)
        for key in all_results:
            all_results[key].append(layer_results[key])

    # ğŸ”¥ æ¸…ç©ºç¼“å­˜
    del model
    torch.cuda.empty_cache()
    print("\nğŸ§¹ CUDA Cache Cleared!\n")

    return {key: np.array(values) for key, values in all_results.items()}

# ğŸš€ è®¡ç®— LLaMA 2 (7B) å’Œ LLaMA 7B çš„æ¨¡å—å½±å“
llama2_7b_results = process_model("LLaMA 2 (7B)", "meta-llama/Llama-2-7b-hf")
llama7b_results = process_model("LLaMA 7B)", "pinkmanlove/llama-7b-hf")

# ğŸš€ è®¡ç®—å„æ¨¡å—çš„å¹³å‡å½±å“
for key in llama2_7b_results:
    print(f"\nğŸ” **{key} Comparison**")
    print(f"LLaMA 2 (7B) {key} (Avg): {np.mean(llama2_7b_results[key]):.4f}")
    print(f"LLaMA 7B {key} (Avg): {np.mean(llama7b_results[key]):.4f}")
